{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from preprocessing import *\n",
    "from models_date import *\n",
    "from models_sex_prediction import *\n",
    "from date_uniformizer import *\n",
    "\n",
    "import sys\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data ...\n",
      "Reading training data : Done\n",
      "Reading testing data ...\n",
      "Reading testing data : Done\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test = read_train_test(sentence_per_row_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_csv(\"X_train.csv\")\n",
    "# y_train.to_csv(\"y_train.csv\")\n",
    "# X_test.to_csv(\"X_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "X_train[\"text_processed\"] = X_train.text.apply(process_text, args=(False,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniformiser les dates (regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"dates_uniformed\"] = X_train.text_processed.apply(uniform_dates_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction du contexte autour de la date (utilisant la target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_to_analyse = extract_X_sentences_before_after(X_train.text_processed[20], y_train.date_accident[20], X=1)\n",
    "\n",
    "\n",
    "# context_to_analyse = extract_X_sentences_around_all_dates(X_train.text_processed[20], \n",
    "#                                                           terms_discarding_the_date, \n",
    "#                                                           use_date_forcing=True,\n",
    "#                                                           date_to_look_for=y_train.date_accident[20])\n",
    "# context_to_analyse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "scrolled": false
   },
   "source": [
    "# Contexte autour de la date d':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Renvoie `None`  si : \n",
    "    - La date dans y n'a pas été trouvée sous le format jour (nombre ou \"1er\") mois (en letter) année : 11 decembre 2007 | 1er avril 2006\n",
    "    - La date est n.c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for i in range(X_train.shape[0]) :\n",
    "    context_to_analyse = extract_X_sentences_around_all_dates(X_train.text_processed.iloc[i], \n",
    "                                                              terms_discarding_the_date, \n",
    "                                                              use_date_forcing=True,\n",
    "                                                              date_to_look_for=y_train.date_accident.iloc[i])\n",
    "    if (context_to_analyse == []) or context_to_analyse == \"Date NC\" :\n",
    "        L.append(None)\n",
    "\n",
    "    else :\n",
    "        L.append(context_to_analyse[0][0]) # je ne prends que le premier contexte pour commencer\n",
    "#         L.append(\" \".join(np.array(context_to_analyse)[:,0]))\n",
    "X_train[\"context_around_accident\"] = L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for i in range(X_train.shape[0]) :\n",
    "    context_to_analyse = extract_X_sentences_around_all_dates(X_train.text_processed.iloc[i], \n",
    "                                                              terms_discarding_the_date, \n",
    "                                                              use_date_forcing=True,\n",
    "                                                              date_to_look_for=y_train.date_consolidation.iloc[i])\n",
    "    if (context_to_analyse == []) or context_to_analyse == \"Date NC\" :\n",
    "        L.append(None)\n",
    "    else :\n",
    "        L.append(context_to_analyse[0][0]) # je ne prends que le premier contexte pour commencer\n",
    "#         L.append(\" \".join(np.array(context_to_analyse)[:,0])) # concatener tout les contextes\n",
    "X_train[\"context_around_consolidation\"] = L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Spacy word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from pathlib import Path\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_tr, X_te = train_test_split(X_train, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To asses the performance of our model that predicts if the context around a date is talking about `date d'accident` or `date de consolidation` :\n",
    "\n",
    "- Take split X_train using train_test_split into X_tr and X_te\n",
    "- Prepare data for model fitting :\n",
    "    - For date d'accident : \n",
    "        - remove the dates from the accident contexts of X_tr\n",
    "        - remove the punctuation from the accident contexts of X_tr\n",
    "        - remove the stopwords from the accident contexts of X_tr  \n",
    "        - use Spacy avg vectorizer or tfidf\n",
    "    - Same thing for date de conso\n",
    "    - concatenate the two \n",
    "    - fit two models : one to predict accident contexts and the other the consolidation contexts\n",
    "- On X_te :\n",
    "    - On each text apply the function extract `X_sentences_around_all_dates`\n",
    "    - for each sentence apply the Spacy vectorizer and predict using the models\n",
    "    - predict the date of which the context gave the higher probability score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train_accident_context_avg_WV = np.array([\n",
    "#     nlp(sen).vector for sen in X_tr.context_around_accident_processed if sen is not None ])\n",
    "\n",
    "# train_consolidation_context_avg_WV = np.array([\n",
    "#     nlp(sen).vector for sen in X_tr.context_around_consolidation_processed if sen is not None ])\n",
    "\n",
    "train_accident_context_avg_WV = np.array([\n",
    "    nlp(sen).vector for sen in X_tr.context_around_accident if sen is not None ])\n",
    "\n",
    "train_consolidation_context_avg_WV = np.array([\n",
    "    nlp(sen).vector for sen in X_tr.context_around_consolidation if sen is not None ])\n",
    "\n",
    "\n",
    " \n",
    "is_accident = [1 for _ in range(train_accident_context_avg_WV.shape[0])]\n",
    "is_not_accident = [0 for _ in range(train_consolidation_context_avg_WV.shape[0])]\n",
    "data = np.concatenate((train_accident_context_avg_WV, train_consolidation_context_avg_WV))\n",
    "target = np.concatenate((is_accident, is_not_accident))\n",
    "\n",
    "# clf_accident = LogisticRegression().fit(data, target)\n",
    "clf_accident = SVC(gamma='auto', probability=True).fit(data, target)\n",
    "\n",
    "###############################################\n",
    "\n",
    "is_conso = [1 for _ in range(train_consolidation_context_avg_WV.shape[0])]\n",
    "is_not_conso = [0 for _ in range(train_accident_context_avg_WV.shape[0])]\n",
    "data = np.concatenate((train_consolidation_context_avg_WV, train_accident_context_avg_WV))\n",
    "target = np.concatenate((is_conso, is_not_conso))\n",
    "\n",
    "# clf_conso = LogisticRegression().fit(data, target)\n",
    "clf_conso = SVC(gamma='auto', probability=True).fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Word_vector_SPACY(text) :\n",
    "    \"\"\"For a given context : gives the average spacy word vectors of the context\n",
    "    used in df.apply() of a 'context_around_...' column of a dataframe\n",
    "    \"\"\"\n",
    "    if text is None :\n",
    "        return None\n",
    "    else :\n",
    "        return nlp(text).vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_accident_context_avg_WV = X_tr.context_around_accident.apply(Word_vector_SPACY)\n",
    "train_consolidation_context_avg_WV = X_tr.context_around_consolidation.apply(Word_vector_SPACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_is_accident_context = np.matrix(train_accident_context_avg_WV[train_accident_context_avg_WV.isnull() == False].to_list())\n",
    "data_is_consolidation_context = np.matrix(train_consolidation_context_avg_WV[train_consolidation_context_avg_WV.isnull() == False].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = np.concatenate((data_is_accident_context, data_is_consolidation_context))\n",
    "target_accident = np.concatenate(([1 for _ in range(data_is_accident_context.shape[0])],\n",
    "                                 [0 for _ in range(data_is_consolidation_context.shape[0])]))\n",
    "\n",
    "target_consolidation = np.concatenate(([0 for _ in range(data_is_accident_context.shape[0])],\n",
    "                                        [1 for _ in range(data_is_consolidation_context.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train_accident_context_avg_WV[train_accident_context_avg_WV.isnull() == False].index\n",
    "# train_consolidation_context_avg_WV[train_consolidation_context_avg_WV.isnull() == False].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf_accident = SVC(gamma='auto', probability=True).fit(data, target_accident)\n",
    "clf_consolidation = SVC(gamma='auto', probability=True).fit(data, target_consolidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def date_prediction_classifier_Word_vectors(text, clf, threshold=0.7):\n",
    "    \n",
    "    probas_ = []\n",
    "    sentences_to_test = extract_X_sentences_around_all_dates(text, terms_discarding_the_date)\n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        WV_processed_sentence = nlp(processed_sentence).vector\n",
    "        probas_.append(clf.predict_proba([WV_processed_sentence]))\n",
    "\n",
    "    probas_ = np.array(probas_)\n",
    "    if probas_.shape[0] == 1 :\n",
    "        probas_ = probas_[0]\n",
    "    else :\n",
    "        probas_ = np.squeeze(probas_)[:,1]\n",
    "    \n",
    "    if probas_.max() >= threshold :\n",
    "        return letter_date_to_submission_date(sentences_to_test[np.argmax(probas_)][1])\n",
    "    else :\n",
    "        return \"n.c.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred_accident = X_te.text_processed.apply(date_prediction_classifier_Word_vectors, args=(clf_accident,))\n",
    "y_pred_consolidation = X_te.text_processed.apply(date_prediction_classifier_Word_vectors, args=(clf_consolidation,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_true_accident = y_train.date_accident.loc[X_te.index]\n",
    "y_true_consolidation = y_train.date_consolidation.loc[X_te.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49740932642487046, 0.41968911917098445)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_accident == y_pred_accident).mean(), (y_pred_consolidation == y_true_consolidation).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Tf idf approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_is_accident_corpus = X_tr.context_around_accident[X_tr.context_around_accident.isnull() == False].to_list()\n",
    "data_is_consolidation_corpus = X_tr.context_around_consolidation[X_tr.context_around_consolidation.isnull() == False].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data = np.concatenate((data_is_accident_corpus, data_is_consolidation_corpus))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_tfidf = vectorizer.fit_transform(data)\n",
    "\n",
    "target_accident = np.concatenate(([1 for _ in range(len(data_is_accident_corpus))],\n",
    "                                 [0 for _ in range(len(data_is_consolidation_corpus))]))\n",
    "\n",
    "target_consolidation = np.concatenate(([0 for _ in range(len(data_is_accident_corpus))],\n",
    "                                        [1 for _ in range(len(data_is_consolidation_corpus))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "clf_accident = SVC(gamma='auto', probability=True).fit(data_tfidf, target_accident)\n",
    "clf_consolidation = SVC(gamma='auto', probability=True).fit(data_tfidf, target_consolidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def date_prediction_classifier_tfidf(text, clf, vectorizer, threshold=0.7):\n",
    "    \n",
    "    probas_ = []\n",
    "    sentences_to_test = extract_X_sentences_around_all_dates(text, terms_discarding_the_date)\n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        tfidf_encoded_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "        probas_.append(clf.predict_proba(tfidf_encoded_processed_sentence))\n",
    "\n",
    "    probas_ = np.array(probas_)\n",
    "    if probas_.shape[0] == 1 :\n",
    "        probas_ = probas_[0][0]\n",
    "        if probas_.max() >= threshold :\n",
    "            return letter_date_to_submission_date(sentences_to_test[0][1])\n",
    "        else :\n",
    "            return \"n.c.\"\n",
    "        \n",
    "    else :\n",
    "        probas_ = np.squeeze(probas_)[:,1]\n",
    "\n",
    "        if probas_.max() >= threshold :\n",
    "            return letter_date_to_submission_date(sentences_to_test[np.argmax(probas_)][1])\n",
    "        else :\n",
    "            return \"n.c.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y_pred_accident = X_te.text_processed.apply(date_prediction_classifier_tfidf, args=(clf_accident, vectorizer,))\n",
    "y_pred_consolidation = X_te.text_processed.apply(date_prediction_classifier_tfidf, args=(clf_consolidation, vectorizer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_true_accident = y_train.date_accident.loc[X_te.index]\n",
    "y_true_consolidation = y_train.date_consolidation.loc[X_te.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5906735751295337, 0.5025906735751295)"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_accident == y_pred_accident).mean(), (y_pred_consolidation == y_true_consolidation).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Three classes approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "terms_discarding_the_date = [\"loi\",\n",
    "                             \"jugement\",\n",
    "                             \"audience\",\n",
    "                             \"publique\",\n",
    "#                              \"juge\",\n",
    "                             \"tribubal\",\n",
    "                             \"decision\",\n",
    "                             \"greffe\",\n",
    "                             \"conclusion\",\n",
    "                             \"ordonnance\",\n",
    "                             ]\n",
    "def extract_X_sentences_around_all_dates_other_dates(text, terms_discarding_the_date= terms_discarding_the_date, \n",
    "                                                     X=1):\n",
    "    \"\"\"\n",
    "    The text in structured format ( like in the column text of X_train)\n",
    "    - Looks for all the dates in the text\n",
    "    - for each date, extract X sentences  before and after this date (sentence meaning rows of the original doc)\n",
    "    - if a row has multiple dates, separate each date in a row (the dates will have the same context) ==> PROBLEM7\n",
    "    - remove the contexts taht contain the words in terms_discarding_the_date (meaning that those are probably other dates) ==> Build a classifier 3 classes after\n",
    "    - Return a list of tuples (context in lower + no stopwords + clean from punct , date)\n",
    "    - this context can be passed to Spacy avg vectorizer to get the avg Word embedding of the sentence\n",
    "    \"\"\"\n",
    "\n",
    "    l = [re.findall(\"\\d{1,2} [a-zéû]{3,9} \\d{4}\", STRING) for STRING in text]\n",
    "    indexes = [i for i in range(len(l)) if len(l[i])!=0]\n",
    "    ll = [(\" \".join(text[i-X+1 : i+X]), l[i]) for i in indexes]\n",
    "\n",
    "    lll = []\n",
    "    for i in range(len(ll)) :\n",
    "        if len([word for word in terms_discarding_the_date if word in ll[i][0]]) > 0 :\n",
    "            if len(ll[i][1]) == 1 :\n",
    "                date = ll[i][1][0]\n",
    "                context_date_removed = ll[i][0].replace(date, '')\n",
    "                lll.append((context_date_removed, date))\n",
    "                \n",
    "#     return \" \".join([lll[ind][0] for ind in range(len(lll))])\n",
    "\n",
    "    ind = np.random.choice(range(len(lll))) # je ne prends qu'un contexte aleatoirement pour commencer\n",
    "    return lll[ind][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Autre dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "for i in range(X_train.shape[0]) :\n",
    "    context_to_analyse = extract_X_sentences_around_all_dates_other_dates(X_train.text_processed.iloc[i])\n",
    "    \n",
    "    context_to_analyse_cleaned = \"\"\n",
    "    for element in context_to_analyse :\n",
    "        if not element.isalnum():\n",
    "            if element == \" \" :\n",
    "                context_to_analyse_cleaned += element \n",
    "        else :\n",
    "            context_to_analyse_cleaned += element \n",
    "    L.append(context_to_analyse_cleaned)\n",
    "    \n",
    "X_train[\"context_around_autre_date_random\"] = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from pathlib import Path\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_tr, X_te = train_test_split(X_train, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Word vectors approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_accident_context_avg_WV = X_tr.context_around_accident.apply(Word_vector_SPACY)\n",
    "train_consolidation_context_avg_WV = X_tr.context_around_consolidation.apply(Word_vector_SPACY)\n",
    "train_other_context_avg_WV = X_tr.context_around_autre_date_random.apply(Word_vector_SPACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_accident_context = np.matrix(train_accident_context_avg_WV[train_accident_context_avg_WV.isnull() == False].to_list())\n",
    "data_consolidation_context = np.matrix(train_consolidation_context_avg_WV[train_consolidation_context_avg_WV.isnull() == False].to_list())\n",
    "data_other_context = np.matrix(train_other_context_avg_WV[train_other_context_avg_WV.isnull() == False].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data = np.concatenate((data_accident_context, data_consolidation_context, data_other_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "is_accident = [0 for _ in range(data_accident_context.shape[0])]\n",
    "is_conso = [1 for _ in range(data_consolidation_context.shape[0])]\n",
    "is_other = [2 for _ in range(data_other_context.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target = np.concatenate((is_accident, is_conso, is_other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto', probability=True).fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def date_prediction_classifier_Word_vectors_multiclass(text, clf, threshold=0.35):\n",
    "    \n",
    "    probas_ = []\n",
    "    sentences_to_test = extract_X_sentences_around_all_dates(text, terms_discarding_the_date)\n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        WV_processed_sentence = nlp(processed_sentence).vector\n",
    "        probas_.append(clf.predict_proba([WV_processed_sentence]))\n",
    "\n",
    "    probas_ = np.array(probas_)\n",
    "    if probas_.shape[0] == 1 :\n",
    "        probas_ = probas_[0][0]\n",
    "        if np.argmax(probas_) == 0 :\n",
    "            return [letter_date_to_submission_date(sentences_to_test[0][1]), \"n.c.\"]\n",
    "        elif np.argmax(probas_) == 1 :\n",
    "            return [\"n.c.\", letter_date_to_submission_date(sentences_to_test[0][1])] \n",
    "        else :\n",
    "            return [\"n.c.\", \"n.c.\"]\n",
    "    else :\n",
    "        probas_ = np.squeeze(probas_)\n",
    "        \n",
    "        event = [\"accident\", \"conso\"]\n",
    "        decisions = []\n",
    "        for row in probas_ :\n",
    "            if np.argmax(row) == 2 :\n",
    "                decisions.append(\"other\")\n",
    "            elif max(row[:2])<= threshold :\n",
    "                decisions.append(\"other\")\n",
    "            else :\n",
    "                decisions.append(event[np.argmax(row[:2])])\n",
    "        \n",
    "        ind_accident = None\n",
    "        ind_conso = None\n",
    "        prev_prob_acc = 0\n",
    "        prev_prob_conso = 0\n",
    "        \n",
    "        for i in range(probas_.shape[0]):\n",
    "            prob_acc = probas_[i,0]\n",
    "            if (decisions[i] == \"accident\") & (prob_acc >= prev_prob_acc) :\n",
    "                ind_accident = i\n",
    "                prev_prob_acc = prob_acc\n",
    "                \n",
    "            prob_conso = probas_[i,1]\n",
    "            if (decisions[i] == \"conso\") & (prob_conso >= prev_prob_conso):\n",
    "                ind_conso = i\n",
    "                prev_prob_conso = prob_conso\n",
    "                \n",
    "        if ind_accident is None :\n",
    "            date_accident = \"n.c.\"\n",
    "        else :\n",
    "            date_accident = letter_date_to_submission_date(sentences_to_test[ind_accident][1])\n",
    "            \n",
    "        if ind_conso is None :\n",
    "            date_conso = \"n.c.\"\n",
    "        else :\n",
    "            date_conso = letter_date_to_submission_date(sentences_to_test[ind_conso][1])\n",
    "        \n",
    "        return [date_accident, date_conso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = X_te.text_processed.apply(date_prediction_classifier_Word_vectors_multiclass, args=(clf,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred_accident = pd.DataFrame(y_pred.values.tolist(), index= y_pred.index)[0]\n",
    "y_pred_consolidation = pd.DataFrame(y_pred.values.tolist(), index= y_pred.index)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_true_accident = y_train.date_accident.loc[X_te.index]\n",
    "y_true_consolidation = y_train.date_consolidation.loc[X_te.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5492227979274611, 0.38341968911917096)"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_accident == y_pred_accident).mean(), (y_pred_consolidation == y_true_consolidation).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Tf idf approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def date_prediction_classifier_tfidf_multiclass(text, clf, vectorizer, threshold=0.35):\n",
    "    \n",
    "    probas_ = []\n",
    "    sentences_to_test = extract_X_sentences_around_all_dates(text, terms_discarding_the_date=[])\n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        tfidf_encoded_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "        probas_.append(clf.predict_proba(tfidf_encoded_processed_sentence))\n",
    "\n",
    "    probas_ = np.array(probas_)\n",
    "    if probas_.shape[0] == 1 :\n",
    "        probas_ = probas_[0][0]\n",
    "        if np.argmax(probas_) == 0 :\n",
    "            return [letter_date_to_submission_date(sentences_to_test[0][1]), \"n.c.\"]\n",
    "        elif np.argmax(probas_) == 1 :\n",
    "            return [\"n.c.\", letter_date_to_submission_date(sentences_to_test[0][1])] \n",
    "        else :\n",
    "            return [\"n.c.\", \"n.c.\"]\n",
    "    else :\n",
    "        probas_ = np.squeeze(probas_)\n",
    "        \n",
    "        event = [\"accident\", \"conso\"]\n",
    "        decisions = []\n",
    "        for row in probas_ :\n",
    "            if np.argmax(row) == 2 :\n",
    "                decisions.append(\"other\")\n",
    "            elif max(row[:2])<= threshold :\n",
    "                decisions.append(\"other\")\n",
    "            else :\n",
    "                decisions.append(event[np.argmax(row[:2])])\n",
    "        \n",
    "        ind_accident = None\n",
    "        ind_conso = None\n",
    "        prev_prob_acc = 0\n",
    "        prev_prob_conso = 0\n",
    "        \n",
    "        for i in range(probas_.shape[0]):\n",
    "            prob_acc = probas_[i,0]\n",
    "            if (decisions[i] == \"accident\") & (prob_acc >= prev_prob_acc) :\n",
    "                ind_accident = i\n",
    "                prev_prob_acc = prob_acc\n",
    "                \n",
    "            prob_conso = probas_[i,1]\n",
    "            if (decisions[i] == \"conso\") & (prob_conso >= prev_prob_conso):\n",
    "                ind_conso = i\n",
    "                prev_prob_conso = prob_conso\n",
    "                \n",
    "        if ind_accident is None :\n",
    "            date_accident = \"n.c.\"\n",
    "        else :\n",
    "            date_accident = letter_date_to_submission_date(sentences_to_test[ind_accident][1])\n",
    "            \n",
    "        if ind_conso is None :\n",
    "            date_conso = \"n.c.\"\n",
    "        else :\n",
    "            date_conso = letter_date_to_submission_date(sentences_to_test[ind_conso][1])\n",
    "        \n",
    "        return [date_accident, date_conso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_is_accident_corpus = X_tr.context_around_accident[X_tr.context_around_accident.isnull() == False].to_list()\n",
    "data_is_consolidation_corpus = X_tr.context_around_consolidation[X_tr.context_around_consolidation.isnull() == False].to_list()\n",
    "data_other_context = X_tr.context_around_autre_date_random[X_tr.context_around_autre_date_random.isnull() == False].to_list()\n",
    "data = np.concatenate((data_is_accident_corpus, data_is_consolidation_corpus, data_other_context))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_tfidf = vectorizer.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "is_accident = [0 for _ in range(len(data_is_accident_corpus))]\n",
    "is_conso = [1 for _ in range(len(data_is_consolidation_corpus))]\n",
    "is_other = [2 for _ in range(len(data_other_context))]\n",
    "target = np.concatenate((is_accident, is_conso, is_other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto', probability=True).fit(data_tfidf, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = X_te.text_processed.apply(date_prediction_classifier_tfidf_multiclass, args=(clf, vectorizer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred_accident = pd.DataFrame(y_pred.values.tolist(), index= y_pred.index)[0]\n",
    "y_pred_consolidation = pd.DataFrame(y_pred.values.tolist(), index= y_pred.index)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_true_accident = y_train.date_accident.loc[X_te.index]\n",
    "y_true_consolidation = y_train.date_consolidation.loc[X_te.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6528497409326425, 0.44041450777202074)"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_accident == y_pred_accident).mean(), (y_pred_consolidation == y_true_consolidation).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After normalizing the dates : BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer, CamembertModel, CamembertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer_ = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "model = CamembertModel.from_pretrained('camembert-base')\n",
    "# model.cuda()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie a été developpée après avoir ecrit le script `date_uniformizer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_extract_context_around_date(text, \n",
    "                                     use_date_forcing=False, \n",
    "                                     date_to_look_for=None,\n",
    "                                     X=1):\n",
    "    \"\"\"\n",
    "    The text in structured format ( like in the column text of X_train)\n",
    "    \n",
    "    - if use_date_forcing=False : Looks for all the dates in the text\n",
    "    - if use_date_forcing=True : Looks for date_to_look_for in the text (using date_parser)\n",
    "    \n",
    "    - for each date, extract X sentences  before and after this date (sentence meaning rows of the original doc)\n",
    "    - if a row has multiple dates, separate each date in a row (the dates will have the same context) ==> PROBLEM\n",
    "    \n",
    "    \n",
    "    - remove the contexts that contain the words in terms_discarding_the_date (to lower the number of contexts to score with the model later)\n",
    "    - Return a list of tuples (context in lower + no stopwords + clean from punct + clan from date)\n",
    "    - this context can be passed to Spacy avg vectorizer to get the avg Word embedding of the sentence or tfidf vectorizer\n",
    "    \"\"\"\n",
    "    if use_date_forcing :\n",
    "        assert date_to_look_for is not None\n",
    "        try :\n",
    "            date = date_parser(date_to_look_for)\n",
    "        except :\n",
    "            return \"Date NC\"\n",
    "        \n",
    "        l = [re.findall(date, STRING)  for STRING in text ] \n",
    "        indexes = [i for i in range(len(l)) if len(l[i])!=0]\n",
    "        ll = [(\" \".join(text[i-X+1 : i+X]), l[i]) for i in indexes]\n",
    "        \n",
    "    \n",
    "    else :\n",
    "        l = [re.findall(\"\\d{1,2} [a-zéû]{3,9} \\d{4}\", STRING) for STRING in text]\n",
    "        indexes = [i for i in range(len(l)) if len(l[i])!=0]\n",
    "        ll = [(\" \".join(text[i-X+1 : i+X]), l[i]) for i in indexes]\n",
    "    return ll\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Façon 2 \n",
    "\n",
    "passer chaque embedding par BERT et moyenner l'embedding du CLS tokens de chaque contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = []\n",
    "for j in range(X_train.shape[0]) :\n",
    "        \n",
    "    context_to_analyse = BERT_extract_context_around_date(X_train.dates_uniformed.iloc[j], \n",
    "                                                          use_date_forcing=True,\n",
    "                                                          date_to_look_for=y_train.date_accident.iloc[j])\n",
    "    if (context_to_analyse == \"Date NC\")or(context_to_analyse==[]) :\n",
    "        L.append(np.nan)\n",
    "    else :\n",
    "        L.append(list(np.array(context_to_analyse)[:,0]))\n",
    "X_train[\"accident_contexts\"] = L\n",
    "(X_train.accident_contexts.isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = []\n",
    "for j in range(X_train.shape[0]) :\n",
    "        \n",
    "    context_to_analyse = BERT_extract_context_around_date(X_train.dates_uniformed.iloc[j], \n",
    "                                                          use_date_forcing=True,\n",
    "                                                          date_to_look_for=y_train.date_consolidation.iloc[j])\n",
    "    if (context_to_analyse == \"Date NC\")or(context_to_analyse==[]) :\n",
    "        L.append(np.nan)\n",
    "    else :\n",
    "        L.append(list(np.array(context_to_analyse)[:,0]))\n",
    "X_train[\"conso_contexts\"] = L\n",
    "(X_train.conso_contexts.isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_tr.date_consolidation == \"n.c.\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour l'evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X_train, y_train, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accident_corpus = X_tr[~X_tr.accident_contexts.isna()].accident_contexts\n",
    "train_conso_corpus = X_tr[~X_tr.accident_contexts.isna()].conso_contexts\n",
    "\n",
    "target_train_accident_corpus = [1 for _ in range(len(train_accident_corpus))] + [0 for _ in range(len(train_conso_corpus))]\n",
    "target_train_conso_corpus = [0 for _ in range(len(train_accident_corpus))] + [1 for _ in range(len(train_conso_corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################       FOR CPU RUNNING ######################\n",
    "  \n",
    "# train_BERT = []\n",
    "\n",
    "# print(\"{} accident contexts to process \".format(len(train_accident_corpus)))\n",
    "# print(\"{} conso contexts to process \".format(len(train_conso_corpus)))\n",
    "\n",
    "# print(\"Processing accidents contexts ...\")\n",
    "# for i, texts in enumerate(train_accident_corpus) :\n",
    "#     sys.stdout.write('\\r'+str(i)+\"/\"+str(len(train_accident_corpus)))\n",
    "#     tokens = tokenizer_.batch_encode_plus(np.array(texts), \n",
    "#                                       add_special_tokens=True,\n",
    "#                                       padding_side='right',\n",
    "#                                       pad_to_max_length=True,\n",
    "#                                       return_tensors='pt')[\"input_ids\"]\n",
    "#     last_hidden_state, pooler_output = model(tokens)\n",
    "#     CLS_tokens = last_hidden_state[:,0,:]\n",
    "#     mean_CLS_tokens=CLS_tokens.mean(axis=0)\n",
    "#     train_BERT.append(mean_CLS_tokens)\n",
    "\n",
    "# print(\"Processing accident context finished !\")\n",
    "\n",
    "# print(\"Processing conso contexts ...\")\n",
    "# for i, texts in enumerate(train_conso_corpus) :\n",
    "#     tokens = tokenizer_.batch_encode_plus(np.array(texts), \n",
    "#                                       add_special_tokens=True,\n",
    "#                                       padding_side='right',\n",
    "#                                       pad_to_max_length=True,\n",
    "#                                       return_tensors='pt')[\"input_ids\"]\n",
    "#     last_hidden_state, pooler_output = model(tokens)\n",
    "#     CLS_tokens = last_hidden_state[:,0,:]\n",
    "#     mean_CLS_tokens=CLS_tokens.mean(axis=0)\n",
    "#     train_BERT.append(mean_CLS_tokens)\n",
    "\n",
    "# print(\"Processing conso context finished !\")\n",
    "\n",
    "# train_BERT = np.array(train_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j = 100\n",
    "# texts = BERT_extract_context_around_date(X_te.dates_uniformed.iloc[j], \n",
    "#                                         use_date_forcing=False)\n",
    "\n",
    "# tokens = tokenizer_.batch_encode_plus(np.array(texts)[:,0], \n",
    "#                                       add_special_tokens=True,\n",
    "#                                       padding_side='right',\n",
    "#                                       pad_to_max_length=True,\n",
    "#                                       return_tensors='pt')[\"input_ids\"]\n",
    "# last_hidden_state, pooler_output = model(tokens)\n",
    "# CLS_tokens = last_hidden_state[:,0,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accident_corpus = X_train[~X_train.accident_contexts.isna()].accident_contexts\n",
    "train_conso_corpus = X_train[~X_train.accident_contexts.isna()].conso_contexts\n",
    "\n",
    "target_train_accident_corpus = [1 for _ in range(len(train_accident_corpus))] + [0 for _ in range(len(train_conso_corpus))]\n",
    "target_train_conso_corpus = [0 for _ in range(len(train_accident_corpus))] + [1 for _ in range(len(train_conso_corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_BERT = []\n",
    "\n",
    "# print(\"{} accident contexts to process \".format(len(train_accident_corpus)))\n",
    "# print(\"{} conso contexts to process \".format(len(train_conso_corpus)))\n",
    "\n",
    "# print(\"Processing accidents contexts ...\")\n",
    "# for i, texts in enumerate(train_accident_corpus) :\n",
    "#     sys.stdout.write('\\r'+str(i)+\"/\"+str(len(train_accident_corpus)))\n",
    "#     tokens = tokenizer_.batch_encode_plus(np.array(texts), \n",
    "#                                       add_special_tokens=True,\n",
    "#                                       padding_side='right',\n",
    "#                                       pad_to_max_length=True,\n",
    "#                                       return_tensors='pt')[\"input_ids\"]\n",
    "#     last_hidden_state, pooler_output = model(tokens.to(\"cuda\"))\n",
    "#     CLS_tokens = last_hidden_state[:,0,:]\n",
    "#     mean_CLS_tokens=CLS_tokens.mean(axis=0).detach().cpu().numpy()\n",
    "#     del last_hidden_state, pooler_output, CLS_tokens\n",
    "#     torch.cuda.empty_cache()\n",
    "#     train_BERT.append(mean_CLS_tokens)\n",
    "\n",
    "# print(\"Processing accident context finished !\")\n",
    "\n",
    "# print(\"Processing conso contexts ...\")\n",
    "# for i, texts in enumerate(train_conso_corpus) :\n",
    "#     tokens = tokenizer_.batch_encode_plus(np.array(texts), \n",
    "#                                       add_special_tokens=True,\n",
    "#                                       padding_side='right',\n",
    "#                                       pad_to_max_length=True,\n",
    "#                                       return_tensors='pt')[\"input_ids\"]\n",
    "#     last_hidden_state, pooler_output = model(tokens.to(\"cuda\"))\n",
    "#     CLS_tokens = last_hidden_state[:,0,:]\n",
    "#     mean_CLS_tokens=CLS_tokens.mean(axis=0).detach().cpu().numpy()\n",
    "#     del last_hidden_state, pooler_output, CLS_tokens\n",
    "#     torch.cuda.empty_cache()\n",
    "#     train_BERT.append(mean_CLS_tokens)\n",
    "\n",
    "# print(\"Processing conso context finished !\")\n",
    "\n",
    "# train_BERT = np.array(train_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "################       FOR CPU RUNNING ######################\n",
    "  \n",
    "# train_BERT = []\n",
    "\n",
    "# print(\"{} accident contexts to process \".format(len(train_accident_corpus)))\n",
    "# print(\"{} conso contexts to process \".format(len(train_conso_corpus)))\n",
    "\n",
    "# print(\"Processing accidents contexts ...\")\n",
    "# for i, texts in enumerate(train_accident_corpus) :\n",
    "#     sys.stdout.write('\\r'+str(i)+\"/\"+str(len(train_accident_corpus)))\n",
    "#     tokens = tokenizer_.batch_encode_plus(np.array(texts), \n",
    "#                                       add_special_tokens=True,\n",
    "#                                       padding_side='right',\n",
    "#                                       pad_to_max_length=True,\n",
    "#                                       return_tensors='pt')[\"input_ids\"]\n",
    "#     last_hidden_state, pooler_output = model(tokens)\n",
    "#     CLS_tokens = last_hidden_state[:,0,:]\n",
    "#     mean_CLS_tokens=CLS_tokens.mean(axis=0)\n",
    "#     train_BERT.append(mean_CLS_tokens)\n",
    "\n",
    "# print(\"Processing accident context finished !\")\n",
    "\n",
    "# print(\"Processing conso contexts ...\")\n",
    "# for i, texts in enumerate(train_conso_corpus) :\n",
    "#     tokens = tokenizer_.batch_encode_plus(np.array(texts), \n",
    "#                                       add_special_tokens=True,\n",
    "#                                       padding_side='right',\n",
    "#                                       pad_to_max_length=True,\n",
    "#                                       return_tensors='pt')[\"input_ids\"]\n",
    "#     last_hidden_state, pooler_output = model(tokens)\n",
    "#     CLS_tokens = last_hidden_state[:,0,:]\n",
    "#     mean_CLS_tokens=CLS_tokens.mean(axis=0)\n",
    "#     train_BERT.append(mean_CLS_tokens)\n",
    "\n",
    "# print(\"Processing conso context finished !\")\n",
    "\n",
    "# train_BERT = np.array(train_BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Submission 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Naive sex predictor version 1 \n",
    "- accident : tfidf 3 classes\n",
    "- consolidation : tf idf 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sex_prediction = X_test.text.apply(naive_sex_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test[\"text_processed\"] = X_test.text.apply(process_text, args=(False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_is_accident_corpus = X_train.context_around_accident[X_train.context_around_accident.isnull() == False].to_list()\n",
    "data_is_consolidation_corpus = X_train.context_around_consolidation[X_train.context_around_consolidation.isnull() == False].to_list()\n",
    "data = np.concatenate((data_is_accident_corpus, data_is_consolidation_corpus))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_tfidf = vectorizer.fit_transform(data)\n",
    "\n",
    "\n",
    "target_consolidation = np.concatenate(([0 for _ in range(len(data_is_accident_corpus))],\n",
    "                                        [1 for _ in range(len(data_is_consolidation_corpus))]))\n",
    "clf_consolidation = SVC(gamma='auto', probability=True).fit(data_tfidf, target_consolidation)\n",
    "y_pred_consolidation = X_test.text_processed.apply(date_prediction_classifier_tfidf, args=(clf_consolidation, vectorizer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_is_accident_corpus = X_train.context_around_accident[X_train.context_around_accident.isnull() == False].to_list()\n",
    "data_is_consolidation_corpus = X_train.context_around_consolidation[X_train.context_around_consolidation.isnull() == False].to_list()\n",
    "data_other_context = X_train.context_around_autre_date_random[X_train.context_around_autre_date_random.isnull() == False].to_list()\n",
    "data = np.concatenate((data_is_accident_corpus, data_is_consolidation_corpus, data_other_context))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_tfidf = vectorizer.fit_transform(data)\n",
    "\n",
    "is_accident = [0 for _ in range(len(data_is_accident_corpus))]\n",
    "is_conso = [1 for _ in range(len(data_is_consolidation_corpus))]\n",
    "is_other = [2 for _ in range(len(data_other_context))]\n",
    "target = np.concatenate((is_accident, is_conso, is_other))\n",
    "clf = SVC(gamma='auto', probability=True).fit(data_tfidf, target)\n",
    "y_pred = X_test.text_processed.apply(date_prediction_classifier_tfidf_multiclass, args=(clf, vectorizer,))\n",
    "y_pred_accident = pd.DataFrame(y_pred.values.tolist(), index= y_pred.index)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sexe</th>\n",
       "      <th>date_accident</th>\n",
       "      <th>date_consolidation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>homme</td>\n",
       "      <td>2014-10-07</td>\n",
       "      <td>n.c.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>homme</td>\n",
       "      <td>1996-07-05</td>\n",
       "      <td>n.c.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>femme</td>\n",
       "      <td>1989-05-11</td>\n",
       "      <td>n.c.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>homme</td>\n",
       "      <td>2001-01-25</td>\n",
       "      <td>n.c.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>homme</td>\n",
       "      <td>1999-05-28</td>\n",
       "      <td>2001-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>homme</td>\n",
       "      <td>2011-05-23</td>\n",
       "      <td>2005-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>femme</td>\n",
       "      <td>1968-05-20</td>\n",
       "      <td>1968-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>femme</td>\n",
       "      <td>1948-05-13</td>\n",
       "      <td>2012-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>homme</td>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>2002-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>homme</td>\n",
       "      <td>1987-04-29</td>\n",
       "      <td>n.c.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sexe date_accident date_consolidation\n",
       "filename                                        \n",
       "770       homme    2014-10-07               n.c.\n",
       "771       homme    1996-07-05               n.c.\n",
       "772       femme    1989-05-11               n.c.\n",
       "773       homme    2001-01-25               n.c.\n",
       "774       homme    1999-05-28         2001-05-28\n",
       "...         ...           ...                ...\n",
       "1022      homme    2011-05-23         2005-04-07\n",
       "1023      femme    1968-05-20         1968-05-20\n",
       "1024      femme    1948-05-13         2012-01-26\n",
       "1025      homme    2000-10-09         2002-09-30\n",
       "1026      homme    1987-04-29               n.c.\n",
       "\n",
       "[257 rows x 3 columns]"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.DataFrame(index = y_pred_accident.index)\n",
    "submit[\"sexe\"] = sex_prediction\n",
    "submit[\"date_accident\"] = y_pred_accident\n",
    "submit[\"date_consolidation\"] = y_pred_consolidation\n",
    "submit.index = submit.index.rename(\"filename\")\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv(\"Sub1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Submission 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "meme date d'accident et date conso que submission 1 mais avec les predicitons de sexe de `Gender.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd\\nimport numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_sex_preds = np.load(\"list_sex_preds_GENDERipynb.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd\\nimport numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub = pd.read_csv(\"Sub1.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sub.sexe == \"homme\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list_sex_preds == \"homme\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8754863813229572"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list_sex_preds == sub.sexe).sum()/len(list_sex_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sub[\"sexe\"] = list_sex_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"Sub2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

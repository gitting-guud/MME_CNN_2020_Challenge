{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data ...\n",
      "Reading training data : Done\n",
      "Reading testing data ...\n",
      "Reading testing data : Done\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test = read_train_test(sentence_per_row_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.date_consolidation = y_train.date_consolidation.str.replace(\"n.a.\", \"n.c.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[\"text_joined\"] = [\" \".join(X_train.text[i]) for i in range(X_train.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('french'))\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def process_text(text, stem=False):\n",
    "    \"\"\" lowercase, removes stopwords, accents and lemmatizes the tokens if stem=True\"\"\"\n",
    "    text_clean = []\n",
    "    for sen in text : \n",
    "        sen = unidecode.unidecode(sen.replace(\"’\", \" \").replace(\",\",\" \").replace(\".\",\" \").replace(\";\",\" \").lower())\n",
    "        sen = sen.replace(\"/ \",\"/\") #some dates are in DD/MM/yyyy format\n",
    "        tokens = sen.split()\n",
    "        if stem :\n",
    "            tokens_no_stpwrd = [stemmer.stem(tok) for tok in tokens if tok not in stop_words]\n",
    "        else :\n",
    "#             tokens_no_stpwrd = [tok for tok in tokens if (tok not in stop_words) & (tok.isalnum())]\n",
    "            tokens_no_stpwrd = [tok for tok in tokens if (tok not in stop_words)]\n",
    "\n",
    "        no_letters = re.sub(' [a-z] ', \" \", \" \".join(tokens_no_stpwrd))\n",
    "        \n",
    "        text_clean.append(no_letters)\n",
    "\n",
    "    return text_clean\n",
    "\n",
    "X_train[\"text_processed\"] = X_train.text.apply(process_text, args=(False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agen_100515.txt</td>\n",
       "      <td>[Le : 12/11/2019, , , Cour d’appel d’Agen , , ...</td>\n",
       "      <td>[, , , cour appel agen, , chambre sociale, , a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agen_1100752.txt</td>\n",
       "      <td>[Le : 12/11/2019, , , Cour d’appel d’Agen , , ...</td>\n",
       "      <td>[, , , cour appel agen, , chambre civile, , au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Agen_1613.txt</td>\n",
       "      <td>[Le : 12/11/2019, , , Cour d’appel d’Agen , , ...</td>\n",
       "      <td>[, , , cour appel agen, , audience publique 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agen_2118.txt</td>\n",
       "      <td>[Le : 12/11/2019, , , Cour d’appel d’Agen , , ...</td>\n",
       "      <td>[, , , cour appel agen, , audience publique 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agen_21229.txt</td>\n",
       "      <td>[Le : 12/11/2019, , , Cour d’appel d’Agen , , ...</td>\n",
       "      <td>[, , , cour appel agen, , audience publique 22...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename                                               text  \\\n",
       "ID                                                                        \n",
       "0    Agen_100515.txt  [Le : 12/11/2019, , , Cour d’appel d’Agen , , ...   \n",
       "1   Agen_1100752.txt  [Le : 12/11/2019, , , Cour d’appel d’Agen , , ...   \n",
       "2      Agen_1613.txt  [Le : 12/11/2019, , , Cour d’appel d’Agen , , ...   \n",
       "3      Agen_2118.txt  [Le : 12/11/2019, , , Cour d’appel d’Agen , , ...   \n",
       "4     Agen_21229.txt  [Le : 12/11/2019, , , Cour d’appel d’Agen , , ...   \n",
       "\n",
       "                                       text_processed  \n",
       "ID                                                     \n",
       "0   [, , , cour appel agen, , chambre sociale, , a...  \n",
       "1   [, , , cour appel agen, , chambre civile, , au...  \n",
       "2   [, , , cour appel agen, , audience publique 26...  \n",
       "3   [, , , cour appel agen, , audience publique 20...  \n",
       "4   [, , , cour appel agen, , audience publique 22...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train.loc[X_train[X_train.filename == \"Bordeaux_605561.txt\"].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction du contexte autour de la date (utilisant la target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def date_parser(date_string):\n",
    "    \"\"\" Transforme une date de YYYY-mm-DD \n",
    "        en DD mm (en lettre) YYYY \n",
    "        pour la rechercher dans le texte\n",
    "    \"\"\"\n",
    "    \n",
    "    year_month_day = date_string.split('-')\n",
    "\n",
    "    list_months=['janvier',\"fevrier\",\n",
    "                 'mars','avril','mai','juin','juillet',\n",
    "                 \"aout\",'septembre','octobre','novembre',\n",
    "                 \"decembre\"]\n",
    "    \n",
    "    day = str(int(year_month_day[2])) # sans le 0 qui précède les unités\n",
    "    month = list_months[int(year_month_day[1])-1]\n",
    "    year = year_month_day[0]\n",
    "    \n",
    "    if day == \"1\":\n",
    "        day +=\"er\"\n",
    "    return day + \" \" + month + \" \" + year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [
     28,
     91,
     95
    ]
   },
   "outputs": [],
   "source": [
    "def extract_X_sentences_before_after(text, date, X=2):\n",
    "    \"\"\" Va chercher X pharases avant et apres la date passée en param dans le texte\n",
    "    peut matcher avec plusieurs dates : renvoit toutes ces phrases\"\"\"\n",
    "    try :\n",
    "        non_capitalized_date = date_parser(date)\n",
    "    except :\n",
    "        return \"Date NC\"\n",
    "    \n",
    "    L = np.array([non_capitalized_date in sen for sen in text]) + np.array([non_capitalized_date[:-5] in sen for sen in text])\n",
    "    \n",
    "    indexes = []\n",
    "    \n",
    "    if np.array(L).any() :\n",
    "        indexes = np.where(L)[0]\n",
    "    else :\n",
    "        return \"Pas trouvé de Date sous format DD mm (en lettre) YYYY dans le texte\"\n",
    "    \n",
    "    result = []\n",
    "    for ind in indexes :\n",
    "        result.append(\" \".join(text[ind-X : ind+X+1]))\n",
    "    \n",
    "    result = [e.replace(non_capitalized_date, \"\").replace(non_capitalized_date[:-5], '') for e in result]\n",
    "    return result\n",
    "\n",
    "import itertools\n",
    "import igraph\n",
    "import copy\n",
    "\n",
    "def terms_to_graph(terms, window_size):\n",
    "    '''This function returns a directed, weighted igraph from lists of list of terms (the tokens from the pre-processed text)\n",
    "    e.g., ['quick','brown','fox']\n",
    "    Edges are weighted based on term co-occurence within a sliding window of fixed size 'w'\n",
    "    '''\n",
    "    \n",
    "    from_to = {}\n",
    "\n",
    "    w = min(window_size, len(terms))\n",
    "    # create initial complete graph (first w terms)\n",
    "    terms_temp = terms[0:w]\n",
    "    indexes = list(itertools.combinations(range(w), r=2))\n",
    "\n",
    "    new_edges = []\n",
    "\n",
    "    for my_tuple in indexes:\n",
    "        new_edges.append(tuple([terms_temp[i] for i in my_tuple]))\n",
    "    for new_edge in new_edges:\n",
    "        if new_edge in from_to:\n",
    "            from_to[new_edge] += 1\n",
    "        else:\n",
    "            from_to[new_edge] = 1\n",
    "\n",
    "    # then iterate over the remaining terms\n",
    "    for i in range(w, len(terms)):\n",
    "        # term to consider\n",
    "        considered_term = terms[i]\n",
    "        # all terms within sliding window\n",
    "        terms_temp = terms[(i - w + 1):(i + 1)]\n",
    "\n",
    "        # edges to try\n",
    "        candidate_edges = []\n",
    "        for p in range(w - 1):\n",
    "            candidate_edges.append((terms_temp[p], considered_term))\n",
    "\n",
    "        for try_edge in candidate_edges:\n",
    "\n",
    "            # if not self-edge\n",
    "            if try_edge[1] != try_edge[0]:\n",
    "\n",
    "                # if edge has already been seen, update its weight\n",
    "                if try_edge in from_to:\n",
    "                    from_to[try_edge] += 1\n",
    "\n",
    "                # if edge has never been seen, create it and assign it a unit weight\n",
    "                else:\n",
    "                    from_to[try_edge] = 1\n",
    "\n",
    "    # create empty graph\n",
    "    g = igraph.Graph(directed=True)\n",
    "\n",
    "    # add vertices\n",
    "    g.add_vertices(sorted(set(terms)))\n",
    "\n",
    "    # add edges, direction is preserved since the graph is directed\n",
    "    g.add_edges(list(from_to.keys()))\n",
    "\n",
    "    # set edge and vertice weights\n",
    "    g.es['weight'] = list(from_to.values()) # based on co-occurence within sliding window\n",
    "    g.vs['weight'] = g.strength(weights=list(from_to.values())) # weighted degree\n",
    "\n",
    "    return (g)\n",
    "\n",
    "def core_dec(g,weighted):\n",
    "    '''(un)weighted k-core decomposition'''\n",
    "    # work on clone of g to preserve g \n",
    "    gg = copy.deepcopy(g)\n",
    "    if not weighted:\n",
    "        gg.vs['weight'] = gg.strength() # overwrite the 'weight' vertex attribute with the unweighted degrees\n",
    "    # initialize dictionary that will contain the core numbers\n",
    "    cores_g = dict(zip(gg.vs['name'],[0]*len(gg.vs)))\n",
    "    \n",
    "    while len(gg.vs) > 0:\n",
    "        # find index of lowest degree vertex\n",
    "        min_degree = min(gg.vs['weight'])\n",
    "        index_top = gg.vs['weight'].index(min_degree)\n",
    "        name_top = gg.vs[index_top]['name']\n",
    "        # get names of its neighbors\n",
    "        neighbors = gg.vs[gg.neighbors(index_top)]['name']\n",
    "        # exclude self-edges\n",
    "        neighbors = [elt for elt in neighbors if elt!=name_top]\n",
    "        # set core number of lowest degree vertex as its degree\n",
    "        cores_g[name_top] = min_degree\n",
    "        # delete top vertex and its incident edges\n",
    "        gg.delete_vertices(index_top)\n",
    "        \n",
    "        if neighbors:\n",
    "            if weighted: \n",
    "                new_degrees = gg.strength(weights=gg.es['weight'])\n",
    "            else:\n",
    "                new_degrees = gg.strength()\n",
    "            # iterate over neighbors of top element\n",
    "            for neigh in neighbors:\n",
    "                index_n = gg.vs['name'].index(neigh)\n",
    "                gg.vs[index_n]['weight'] = max(min_degree,new_degrees[index_n])  \n",
    "        \n",
    "    return(cores_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acte axa france iard nouvelle denomination sociale place compagnie assurances jugement defere reconnu droit philippe indemnisation integrale prejudice suite accident  surplus jugement defere statuant nouveau',\n",
       " 'incidence professionnelle temporaire attendu evaluer incidence professionnelle temporaire periode  10 juin 1999 arret mixte precite demande expert judiciaire calculer montant prejudice nature economique effectivement subi philippe compte tenu revenus professionnels anterieurs a attendu expert precise rapport revenu professionnel philippe anterieurement accident correspond benefice net fiscal annuel realise cadre exploitation fonds commerce salon the situe menton acquis 16 mai',\n",
       " 'attendu toutefois expert indique 000 subie vente fonds commerce realisee 29 octobre 1999 consequence chute constante chiffre affaires patisserie depuis accident  cette baisse representant chiffre affaire exercice 1994 exercice complet avant rapport dernier exercice clos avant baisse chiffre affaires 909 EUR 35 vente philippe apprenant 10 juin 1999 peut plus exercer metier patissier vendant fonds commerce 29 octobre 1999 intermediaire cabinet specialise dont ignore veritables demarches effectuees mise dit fonds commerce marche 714 EUR 31 alors fonds commerce existe depuis 1848 plus repute ville situe zone']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_to_analyse = extract_X_sentences_before_after(X_train.text_processed[20], y_train.date_accident[20], X=1)\n",
    "context_to_analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sexe                       homme\n",
       "date_accident         1996-11-26\n",
       "date_consolidation          n.c.\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = terms_to_graph(words.split(\" \"), 4)\n",
    "# dico = core_dec(g, True)\n",
    "# dico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Contexte autour de la date d':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_discarding_the_date = [\"loi\",\n",
    "                             \"jugement\",\n",
    "                             \"audience\",\n",
    "                             \"publique\",\n",
    "                             \"tribubal\",\n",
    "                             \"decision\",\n",
    "                             \"greffe\",\n",
    "                             \"ordonnance\",\n",
    "                             ]\n",
    "def extract_X_sentences_around_all_dates(text, terms_discarding_the_date=terms_discarding_the_date, \n",
    "                                         use_date_forcing=False, date_to_look_for=None,\n",
    "                                         X=1):\n",
    "    \"\"\"\n",
    "    The text in structured format ( like in the column text of X_train)\n",
    "    - Looks for all the dates in the text\n",
    "    - for each date, extract X sentences  before and after this date (sentence meaning rows of the original doc)\n",
    "    - if a row has multiple dates, separate each date in a row (the dates will have the same context) ==> PROBLEM7\n",
    "    - remove the contexts taht contain the words in terms_discarding_the_date (meaning that those are probably other dates) ==> Build a classifier 3 classes after\n",
    "    - Return a list of tuples (context in lower + no stopwords + clean from punct , date)\n",
    "    - this context can be passed to Spacy avg vectorizer to get the avg Word embedding of the sentence\n",
    "    \"\"\"\n",
    "    if use_date_forcing :\n",
    "        assert date_to_look_for is not None\n",
    "        try :\n",
    "            date = date_parser(date_to_look_for)\n",
    "        except :\n",
    "            return \"Date NC\"\n",
    "        \n",
    "        l = [re.findall(date, STRING) if (re.findall(date, STRING)!=[]) else re.findall(date[:-5], STRING) for STRING in text ] \n",
    "        indexes = [i for i in range(len(l)) if len(l[i])!=0]\n",
    "        ll = [(\" \".join(text[i-X+1 : i+X]), l[i]) for i in indexes]\n",
    "        \n",
    "        lll = []\n",
    "        for i in range(len(ll)) :\n",
    "            if len([word for word in terms_discarding_the_date if word in ll[i][0]]) == 0 :\n",
    "                if len(ll[i][1]) == 1 :\n",
    "                    date = ll[i][1][0]\n",
    "                    context_date_removed = ll[i][0].replace(date, '')\n",
    "                    context_date_removed_cleaned = \"\"\n",
    "                    for element in context_date_removed :\n",
    "                        if not element.isalnum():\n",
    "                            if element == \" \" :\n",
    "                                context_date_removed_cleaned += element \n",
    "                        else :\n",
    "                            context_date_removed_cleaned += element \n",
    "                    lll.append((context_date_removed_cleaned, date))\n",
    "                else :\n",
    "                    for j in range(len(ll[i][1])) :\n",
    "                        date = ll[i][1][j]\n",
    "                        context_date_removed = ll[i][0].replace(date, '')\n",
    "                        context_date_removed_cleaned = ''\n",
    "                        for element in context_date_removed :\n",
    "                            if not element.isalnum():\n",
    "                                if element == \" \" :\n",
    "                                    context_date_removed_cleaned += element \n",
    "                            else :\n",
    "                                context_date_removed_cleaned += element \n",
    "                        lll.append((context_date_removed_cleaned, date))\n",
    "    else :\n",
    "        l = [re.findall(\"\\d{1,2} [a-zéû]{3,9} \\d{4}\", STRING) for STRING in text]\n",
    "        indexes = [i for i in range(len(l)) if len(l[i])!=0]\n",
    "        ll = [(\" \".join(text[i-X+1 : i+X]), l[i]) for i in indexes]\n",
    "        lll = []\n",
    "        for i in range(len(ll)) :\n",
    "            if len([word for word in terms_discarding_the_date if word in ll[i][0]]) == 0 :\n",
    "                if len(ll[i][1]) == 1 :\n",
    "                    date = ll[i][1][0]\n",
    "                    context_date_removed = ll[i][0].replace(date, '')\n",
    "                    if context_date_removed != \"\": \n",
    "                        lll.append((context_date_removed, date))\n",
    "                else :\n",
    "                    for j in range(len(ll[i][1])) :\n",
    "                        date = ll[i][1][j]\n",
    "                        context_date_removed = ll[i][0].replace(date, '')\n",
    "                        if context_date_removed != \"\": \n",
    "                            lll.append((context_date_removed, date))\n",
    "    return lll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms_discarding_the_date = [\"loi\",\n",
    "#                              \"jugement\",\n",
    "#                              \"audience\",\n",
    "#                              \"publique\",\n",
    "#                              \"juge\",\n",
    "#                              \"tribubal\",\n",
    "#                              \"decision\",\n",
    "#                              \"greffe\",\n",
    "#                              \"conclusion\",\n",
    "#                              \"ordonnance\",\n",
    "#                              ]\n",
    "\n",
    "# def extract_X_sentences_around_all_dates(text, terms_discarding_the_date, X=1):\n",
    "#     \"\"\"\n",
    "#     The text in structured format ( like in the column text of X_train)\n",
    "#     - Looks for all the dates in the text\n",
    "#     - for each date, extract X sentences  before and after this date (sentence meaning rows of the original doc)\n",
    "#     - if a row has multiple dates, separate each date in a row (the dates will have the same context) ==> PROBLEM\n",
    "#     - Return a list of tuples (context in lower + no stopwords + clean from punct , date)\n",
    "#     - this context can be passed to Spacy avg vectorizer to get the avg Word embedding of the sentence\n",
    "#     \"\"\"\n",
    "#     l = [re.findall(\"\\d{1,2} [a-zéû]{3,9} \\d{4}\", STRING) for STRING in text]\n",
    "#     indexes = [i for i in range(len(l)) if len(l[i])!=0]\n",
    "#     ll = [(\" \".join(text[i-X+1 : i+X]), l[i]) for i in indexes]\n",
    "    \n",
    "    \n",
    "#     lll = []\n",
    "#     for i in range(len(ll)) :\n",
    "#         if len([word for word in terms_discarding_the_date if word in ll[i][0]]) == 0 :\n",
    "#             if len(ll[i][1]) == 1 :\n",
    "#                 date = ll[i][1][0]\n",
    "#                 context_date_removed = ll[i][0].replace(date, '')\n",
    "#                 lll.append((context_date_removed, date))\n",
    "#             else :\n",
    "#                 for j in range(len(ll[i][1])) :\n",
    "\n",
    "#                     date = ll[i][1][j]\n",
    "#                     context_date_removed = ll[i][0].replace(date, '')\n",
    "#                     lll.append((context_date_removed, date))\n",
    "#     return lll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for i in range(X_train.shape[0]) :\n",
    "#     context_to_analyse = extract_X_sentences_before_after(X_train.text_processed[i], y_train.date_accident[i], X=1)\n",
    "    context_to_analyse = extract_X_sentences_around_all_dates(X_train.text_processed.iloc[i], \n",
    "                                                              terms_discarding_the_date, \n",
    "                                                              use_date_forcing=True,\n",
    "                                                              date_to_look_for=y_train.date_accident.iloc[i])\n",
    "    if (context_to_analyse == []) or context_to_analyse == \"Date NC\" :\n",
    "        L.append(None)\n",
    "#     if context_to_analyse in [\"Pas trouvé de Date sous format DD mm (en lettre) YYYY dans le texte\", \"Date NC\"] :\n",
    "#         L.append(context_to_analyse)\n",
    "    else :\n",
    "        L.append(context_to_analyse[0][0]) # je ne prends que le premier contexte pour commencer\n",
    "#         L.append(\" \".join(context_to_analyse))\n",
    "X_train[\"context_around_accident\"] = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = []\n",
    "# for i in range(X_train.shape[0]) :\n",
    "#     text = X_train.context_around_accident[i]\n",
    "#     if text in [\"Pas trouvé de Date sous format DD mm (en lettre) YYYY dans le texte\", \"Date NC\"] :\n",
    "#         L.append(None)\n",
    "#     else :\n",
    "#         L.append(text)\n",
    "# X_train[\"context_around_accident_processed\"] = L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for i in range(X_train.shape[0]) :\n",
    "#     context_to_analyse = extract_X_sentences_before_after(X_train.text_processed[i], y_train.date_consolidation[i], X=1)\n",
    "#     if context_to_analyse in [\"Pas trouvé de Date sous format DD mm (en lettre) YYYY dans le texte\", \"Date NC\"] :\n",
    "#         L.append(context_to_analyse)\n",
    "    context_to_analyse = extract_X_sentences_around_all_dates(X_train.text_processed.iloc[i], \n",
    "                                                              terms_discarding_the_date, \n",
    "                                                              use_date_forcing=True,\n",
    "                                                              date_to_look_for=y_train.date_consolidation.iloc[i])\n",
    "    if (context_to_analyse == []) or context_to_analyse == \"Date NC\" :\n",
    "        L.append(None)\n",
    "    else :\n",
    "        L.append(context_to_analyse[0][0]) # je ne prends que le premier contexte pour commencer\n",
    "#         L.append(\" \".join(context_to_analyse))\n",
    "X_train[\"context_around_consolidation\"] = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = []\n",
    "# for i in range(X_train.shape[0]) :\n",
    "#     text = X_train.context_around_consolidation[i]\n",
    "#     if text in [\"Pas trouvé de Date sous format DD mm (en lettre) YYYY dans le texte\", \"Date NC\"] :\n",
    "#         L.append(None)\n",
    "#     else :\n",
    "#         L.append(text)\n",
    "# X_train[\"context_around_consolidation_processed\"] = L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Spacy word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import fr_core_news_md\n",
    "nlp = fr_core_news_md.load()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "code_folding": [
     2
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "def letter_date_to_submission_date(string) :\n",
    "    \n",
    "    dico_months = {\"janvier\" : \"01\",\n",
    "               'fevrier' : \"02\",\n",
    "               \"mars\" : \"03\",\n",
    "               \"avril\" : \"04\",\n",
    "               \"mai\" : \"05\",\n",
    "               \"juin\" : \"06\",\n",
    "               \"juillet\" : \"07\",\n",
    "               \"aout\" : \"08\",\n",
    "                \"septembre\" : \"09\",\n",
    "               \"octobre\": '10',\n",
    "               \"novembre\" : \"11\",\n",
    "               \"decembre\":'12'}\n",
    "    \n",
    "    strings = string.split()\n",
    "    \n",
    "    if len(strings[0]) == 1 :\n",
    "        day = \"0\"+strings[0]\n",
    "    else :\n",
    "        day = strings[0]\n",
    "        \n",
    "    month = process.extract(strings[1], dico_months.keys())[0][0]\n",
    "    return strings[2]+\"-\"+dico_months[month]+\"-\"+day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To asses the performance of our model that predicts if the context around a date is talking about `date d'accident` or `date de consolidation` :\n",
    "\n",
    "- Take split X_train using train_test_split into X_tr and X_te\n",
    "- Prepare data for model fitting :\n",
    "    - For date d'accident : \n",
    "        - remove the dates from the accident contexts of X_tr\n",
    "        - remove the punctuation from the accident contexts of X_tr\n",
    "        - remove the stopwords from the accident contexts of X_tr  \n",
    "        - use Spacy avg vectorizer or tfidf\n",
    "    - Same thing for date de conso\n",
    "    - concatenate the two \n",
    "    - fit two models : one to predict accident contexts and the other the consolidation contexts\n",
    "- On X_te :\n",
    "    - On each text apply the function extract `X_sentences_around_all_dates`\n",
    "    - for each sentence apply the Spacy vectorizer and predict using the models\n",
    "    - predict the date of which the context gave the higher probability score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Results : \n",
    "- accident : 35% accuracy with tf idf approach on context extracted with X=1 and SVM classifier\n",
    "- conso : 24% accuracy with spacy word vectors approach on context extracted with X=1 and SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import GradientBoostingRegressor\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom pathlib import Path\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_tr, X_te = train_test_split(X_train, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train_accident_context_avg_WV = np.array([\n",
    "#     nlp(sen).vector for sen in X_tr.context_around_accident_processed if sen is not None ])\n",
    "\n",
    "# train_consolidation_context_avg_WV = np.array([\n",
    "#     nlp(sen).vector for sen in X_tr.context_around_consolidation_processed if sen is not None ])\n",
    "\n",
    "train_accident_context_avg_WV = np.array([\n",
    "    nlp(sen).vector for sen in X_tr.context_around_accident if sen is not None ])\n",
    "\n",
    "train_consolidation_context_avg_WV = np.array([\n",
    "    nlp(sen).vector for sen in X_tr.context_around_consolidation if sen is not None ])\n",
    "\n",
    "\n",
    " \n",
    "is_accident = [1 for _ in range(train_accident_context_avg_WV.shape[0])]\n",
    "is_not_accident = [0 for _ in range(train_consolidation_context_avg_WV.shape[0])]\n",
    "data = np.concatenate((train_accident_context_avg_WV, train_consolidation_context_avg_WV))\n",
    "target = np.concatenate((is_accident, is_not_accident))\n",
    "\n",
    "# clf_accident = LogisticRegression().fit(data, target)\n",
    "clf_accident = SVC(gamma='auto', probability=True).fit(data, target)\n",
    "\n",
    "###############################################\n",
    "\n",
    "is_conso = [1 for _ in range(train_consolidation_context_avg_WV.shape[0])]\n",
    "is_not_conso = [0 for _ in range(train_accident_context_avg_WV.shape[0])]\n",
    "data = np.concatenate((train_consolidation_context_avg_WV, train_accident_context_avg_WV))\n",
    "target = np.concatenate((is_conso, is_not_conso))\n",
    "\n",
    "# clf_conso = LogisticRegression().fit(data, target)\n",
    "clf_conso = SVC(gamma='auto', probability=True).fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524, 300)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_accident_context_avg_WV), len(train_consolidation_context_avg_WV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# index_not_none_accident = [ind for ind in X_te.index if X_te.context_around_accident_processed.loc[ind] is not None]\n",
    "# index_not_none_conso = [ind for ind in X_te.index if X_te.context_around_consolidation_processed.loc[ind] is not None]\n",
    "\n",
    "index_not_none_accident = [ind for ind in X_te.index if X_te.context_around_accident.loc[ind] is not None]\n",
    "index_not_none_conso = [ind for ind in X_te.index if X_te.context_around_consolidation.loc[ind] is not None]\n",
    "\n",
    "\n",
    "y_true_accident = y_train.loc[index_not_none_accident].date_accident.values\n",
    "y_true_consolidation = y_train.loc[index_not_none_conso].date_consolidation.values\n",
    "\n",
    "y_pred_accident = {}\n",
    "y_pred_conso = {}\n",
    "\n",
    "for i in X_te.index :\n",
    "    \n",
    "    use_acccident_model = False\n",
    "    use_conso_model = False\n",
    "    \n",
    "    if i in index_not_none_accident :\n",
    "        use_acccident_model = True\n",
    "        \n",
    "    if i in index_not_none_conso :\n",
    "        use_conso_model = True\n",
    "    \n",
    "    sentences_to_test = extract_X_sentences_around_all_dates(X_te.text_processed.loc[i], terms_discarding_the_date)\n",
    "    \n",
    "    probas_accident = []\n",
    "    probas_conso = []\n",
    "    \n",
    "    if use_acccident_model & use_conso_model :\n",
    "        for tup in sentences_to_test :\n",
    "            processed_sentence = tup[0]\n",
    "            WV_processed_sentence = nlp(processed_sentence).vector\n",
    "            probas_accident.append(clf_accident.predict_proba([WV_processed_sentence]))\n",
    "            probas_conso.append(clf_conso.predict_proba([WV_processed_sentence]))\n",
    "    elif use_acccident_model : \n",
    "        for tup in sentences_to_test :\n",
    "            processed_sentence = tup[0]\n",
    "            WV_processed_sentence = nlp(processed_sentence).vector\n",
    "            probas_accident.append(clf_accident.predict_proba([WV_processed_sentence]))\n",
    "    elif use_conso_model :\n",
    "        for tup in sentences_to_test :\n",
    "            processed_sentence = tup[0]\n",
    "            WV_processed_sentence = nlp(processed_sentence).vector\n",
    "            probas_conso.append(clf_conso.predict_proba([WV_processed_sentence]))\n",
    "    else :\n",
    "        pass\n",
    "            \n",
    "    threshold_accident = 0.7\n",
    "    threshold_conso = 0.7    \n",
    "\n",
    "    if use_acccident_model :\n",
    "        probas_accident = np.squeeze(probas_accident)[:,1]\n",
    "        if probas_accident.max() >= threshold_accident :\n",
    "            y_pred_accident[i] = letter_date_to_submission_date(sentences_to_test[np.argmax(probas_accident)][1])\n",
    "        else :\n",
    "            y_pred_accident[i] = \"n.c.\"\n",
    "            \n",
    "    if use_conso_model :\n",
    "        probas_conso = np.squeeze(probas_conso)[:,1]\n",
    "        if probas_conso.max() >= threshold_conso :\n",
    "            y_pred_conso[i] = letter_date_to_submission_date(sentences_to_test[np.argmax(probas_conso)][1])\n",
    "        else :\n",
    "            y_pred_conso[i] = \"n.c.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5535714285714286"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_accident == np.array(list(y_pred_accident.values()))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5252525252525253"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_consolidation == np.array(list(y_pred_conso.values()))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('compagnie assurances generali procede reglement cette somme ', '17 fevrier 2012') 0.8737924124133967 2005-03-05\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import GradientBoostingRegressor\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom pathlib import Path\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import GradientBoostingRegressor\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom pathlib import Path\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import GradientBoostingRegressor\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom pathlib import Path\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('- date consolidation : ', '31 janvier 2009') 0.9999999887316658 2009-01-31\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import GradientBoostingRegressor\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom pathlib import Path\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import GradientBoostingRegressor\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom pathlib import Path\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.choice(X_te.index)\n",
    "\n",
    "y_true_accident_ = dict(y_train.loc[index_not_none_accident].date_accident)\n",
    "y_true_consolidation_ = dict(y_train.loc[index_not_none_conso].date_consolidation)\n",
    "\n",
    "\n",
    "use_acccident_model = False\n",
    "use_conso_model = False\n",
    "\n",
    "if i in index_not_none_accident :\n",
    "    use_acccident_model = True\n",
    "\n",
    "if i in index_not_none_conso :\n",
    "    use_conso_model = True\n",
    "\n",
    "sentences_to_test = extract_X_sentences_around_all_dates(X_te.text_processed.loc[i], terms_discarding_the_date)\n",
    "\n",
    "probas_accident = []\n",
    "probas_conso = []\n",
    "\n",
    "if use_acccident_model & use_conso_model :\n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        WV_processed_sentence = nlp(processed_sentence).vector\n",
    "        probas_accident.append(clf_accident.predict_proba([WV_processed_sentence]))\n",
    "        probas_conso.append(clf_conso.predict_proba([WV_processed_sentence]))\n",
    "elif use_acccident_model : \n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        WV_processed_sentence = nlp(processed_sentence).vector\n",
    "        probas_accident.append(clf_accident.predict_proba([WV_processed_sentence]))\n",
    "elif use_conso_model :\n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        WV_processed_sentence = nlp(processed_sentence).vector\n",
    "        probas_conso.append(clf_conso.predict_proba([WV_processed_sentence]))\n",
    "\n",
    "threshold_accident = 0.7\n",
    "threshold_conso = 0.7    \n",
    "\n",
    "if use_acccident_model :\n",
    "    probas_accident = np.squeeze(probas_accident)[:,1]\n",
    "    print(sentences_to_test[np.argmax(probas_accident)], probas_accident.max(), dict(y_true_accident_)[i])\n",
    "    plt.title(\"probas of accident date context of the index {}\".format(i))\n",
    "    plt.bar(range(probas_accident.shape[0]), probas_accident);\n",
    "    plt.show()\n",
    "if use_conso_model :\n",
    "    probas_conso = np.squeeze(probas_conso)[:,1]\n",
    "    print(sentences_to_test[np.argmax(probas_conso)], probas_conso.max(), dict(y_true_consolidation_)[i])\n",
    "    plt.title(\"probas of consolidation date context of the index {}\".format(i))\n",
    "    plt.bar(range(probas_conso.shape[0]), probas_conso);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf idf approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_accident_corpus = [sen for sen in X_tr.context_around_accident_processed if sen is not None ]\n",
    "# train_consolidation_corpus = [sen for sen in X_tr.context_around_consolidation_processed if sen is not None ]\n",
    "\n",
    "train_accident_corpus = [sen for sen in X_tr.context_around_accident if sen is not None ]\n",
    "train_consolidation_corpus = [sen for sen in X_tr.context_around_consolidation if sen is not None ]\n",
    "\n",
    "data = np.concatenate((train_accident_corpus, train_consolidation_corpus))\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_tfidf = vectorizer.fit_transform(data)\n",
    "\n",
    "\n",
    "is_accident = [1 for _ in range(len(train_accident_corpus))]\n",
    "is_not_accident = [0 for _ in range(len(train_consolidation_corpus))]\n",
    "target = np.concatenate((is_accident, is_not_accident))\n",
    "# clf_accident = LogisticRegression().fit(data, target)\n",
    "clf_accident = SVC(gamma='auto', probability=True).fit(data_tfidf, target)\n",
    "\n",
    "\n",
    "###############################################\n",
    "\n",
    "is_conso = [1 for _ in range(len(train_accident_corpus))]\n",
    "is_not_conso = [0 for _ in range(len(train_consolidation_corpus))]\n",
    "data = np.concatenate((train_consolidation_corpus, train_accident_corpus))\n",
    "target = np.concatenate((is_conso, is_not_conso))\n",
    "# clf_conso = LogisticRegression().fit(data, target)\n",
    "clf_conso = SVC(gamma='auto', probability=True).fit(data_tfidf, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_not_none_accident = [ind for ind in X_te.index if X_te.context_around_accident_processed.loc[ind] is not None]\n",
    "# index_not_none_conso = [ind for ind in X_te.index if X_te.context_around_consolidation_processed.loc[ind] is not None]\n",
    "\n",
    "index_not_none_accident = [ind for ind in X_te.index if X_te.context_around_accident.loc[ind] is not None]\n",
    "index_not_none_conso = [ind for ind in X_te.index if X_te.context_around_consolidation.loc[ind] is not None]\n",
    "\n",
    "\n",
    "y_true_accident = y_train.loc[index_not_none_accident].date_accident.values\n",
    "y_true_consolidation = y_train.loc[index_not_none_conso].date_consolidation.values\n",
    "\n",
    "y_pred_accident = {}\n",
    "y_pred_conso = {}\n",
    "\n",
    "for i in X_te.index :\n",
    "    \n",
    "    use_acccident_model = False\n",
    "    use_conso_model = False\n",
    "    \n",
    "    if i in index_not_none_accident :\n",
    "        use_acccident_model = True\n",
    "        \n",
    "    if i in index_not_none_conso :\n",
    "        use_conso_model = True\n",
    "    \n",
    "    sentences_to_test = extract_X_sentences_around_all_dates(X_te.text_processed.loc[i], terms_discarding_the_date)\n",
    "    \n",
    "    probas_accident = []\n",
    "    probas_conso = []\n",
    "    \n",
    "    if use_acccident_model & use_conso_model :\n",
    "        for tup in sentences_to_test :\n",
    "            processed_sentence = tup[0]\n",
    "            WV_processed_sentence = nlp(processed_sentence).vector\n",
    "            tfidf_encoded_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "            probas_accident.append(clf_accident.predict_proba(tfidf_encoded_processed_sentence))\n",
    "            probas_conso.append(clf_conso.predict_proba(tfidf_encoded_processed_sentence))\n",
    "            \n",
    "    elif use_acccident_model : \n",
    "        for tup in sentences_to_test :\n",
    "            processed_sentence = tup[0]\n",
    "            WV_processed_sentence = nlp(processed_sentence).vector\n",
    "            tfidf_encoded_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "            probas_accident.append(clf_accident.predict_proba(tfidf_encoded_processed_sentence))\n",
    "    elif use_conso_model :\n",
    "        for tup in sentences_to_test :\n",
    "            processed_sentence = tup[0]\n",
    "            WV_processed_sentence = nlp(processed_sentence).vector\n",
    "            tfidf_encoded_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "            probas_conso.append(clf_conso.predict_proba(tfidf_encoded_processed_sentence))\n",
    "    else :\n",
    "        pass\n",
    "            \n",
    "    threshold_accident = 0.7\n",
    "    threshold_conso = 0.8\n",
    "\n",
    "    if use_acccident_model :\n",
    "        probas_accident = np.squeeze(probas_accident)[:,1]\n",
    "        if probas_accident.max() >= threshold_accident :\n",
    "            y_pred_accident[i] = letter_date_to_submission_date(sentences_to_test[np.argmax(probas_accident)][1])\n",
    "        else :\n",
    "            y_pred_accident[i] = \"n.c.\"\n",
    "            \n",
    "    if use_conso_model :\n",
    "        probas_conso = np.squeeze(probas_conso)[:,1]\n",
    "        if probas_conso.max() >= threshold_conso :\n",
    "            y_pred_conso[i] = letter_date_to_submission_date(sentences_to_test[np.argmax(probas_conso)][1])\n",
    "        else :\n",
    "            y_pred_conso[i] = \"n.c.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6607142857142857"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_accident == np.array(list(y_pred_accident.values()))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010101010101010102"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_consolidation == np.array(list(y_pred_conso.values()))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' monsieur pierre circulait motocyclette rn 193 sens ponte leccia-corte (haute corse) ete victime grave accident circulation impliquant vehicule location appartenant holcar assure aupres covea fleet conduit monsieur gerhard h', '21 aout 2003') 0.9443406278082515 2003-08-21\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import GradientBoostingRegressor\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom pathlib import Path\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import GradientBoostingRegressor\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom pathlib import Path\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import GradientBoostingRegressor\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom pathlib import Path\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' monsieur pierre circulait motocyclette rn 193 sens ponte leccia-corte (haute corse) ete victime grave accident circulation impliquant vehicule location appartenant holcar assure aupres covea fleet conduit monsieur gerhard h', '21 aout 2003') 0.9423745951042248 2005-08-21\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import GradientBoostingRegressor\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom pathlib import Path\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import GradientBoostingRegressor\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom pathlib import Path\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.choice(X_te.index)\n",
    "\n",
    "y_true_accident_ = dict(y_train.loc[index_not_none_accident].date_accident)\n",
    "y_true_consolidation_ = dict(y_train.loc[index_not_none_conso].date_consolidation)\n",
    "\n",
    "use_acccident_model = False\n",
    "use_conso_model = False\n",
    "\n",
    "if i in index_not_none_accident :\n",
    "    use_acccident_model = True\n",
    "\n",
    "if i in index_not_none_conso :\n",
    "    use_conso_model = True\n",
    "\n",
    "sentences_to_test = extract_X_sentences_around_all_dates(X_te.text_processed.loc[i], terms_discarding_the_date)\n",
    "\n",
    "probas_accident = []\n",
    "probas_conso = []\n",
    "\n",
    "if use_acccident_model & use_conso_model :\n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        WV_processed_sentence = nlp(processed_sentence).vector\n",
    "        tfidf_encoded_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "        probas_accident.append(clf_accident.predict_proba(tfidf_encoded_processed_sentence))\n",
    "        probas_conso.append(clf_conso.predict_proba(tfidf_encoded_processed_sentence))\n",
    "\n",
    "elif use_acccident_model : \n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        WV_processed_sentence = nlp(processed_sentence).vector\n",
    "        tfidf_encoded_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "        probas_accident.append(clf_accident.predict_proba(tfidf_encoded_processed_sentence))\n",
    "elif use_conso_model :\n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        WV_processed_sentence = nlp(processed_sentence).vector\n",
    "        tfidf_encoded_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "        probas_conso.append(clf_conso.predict_proba(tfidf_encoded_processed_sentence))\n",
    "else :\n",
    "    pass\n",
    "\n",
    "threshold_accident = 0.7\n",
    "threshold_conso = 0.7    \n",
    "\n",
    "if use_acccident_model :\n",
    "    probas_accident = np.squeeze(probas_accident)[:,1]\n",
    "    print(sentences_to_test[np.argmax(probas_accident)], probas_accident.max(), dict(y_true_accident_)[i])\n",
    "    plt.title(\"probas of accident date context of the index {}\".format(i))\n",
    "    plt.bar(range(probas_accident.shape[0]), probas_accident);\n",
    "    plt.show()\n",
    "if use_conso_model :\n",
    "    probas_conso = np.squeeze(probas_conso)[:,1]\n",
    "    print(sentences_to_test[np.argmax(probas_conso)], probas_conso.max(), dict(y_true_consolidation_)[i])\n",
    "    plt.title(\"probas of consolidation date context of the index {}\".format(i))\n",
    "    plt.bar(range(probas_conso.shape[0]), probas_conso);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three classes approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_discarding_the_date = [\"loi\",\n",
    "                             \"jugement\",\n",
    "                             \"audience\",\n",
    "                             \"publique\",\n",
    "#                              \"juge\",\n",
    "                             \"tribubal\",\n",
    "                             \"decision\",\n",
    "                             \"greffe\",\n",
    "                             \"conclusion\",\n",
    "                             \"ordonnance\",\n",
    "                             ]\n",
    "def extract_X_sentences_around_all_dates_other_dates(text, terms_discarding_the_date= terms_discarding_the_date, \n",
    "                                                     X=1):\n",
    "    \"\"\"\n",
    "    The text in structured format ( like in the column text of X_train)\n",
    "    - Looks for all the dates in the text\n",
    "    - for each date, extract X sentences  before and after this date (sentence meaning rows of the original doc)\n",
    "    - if a row has multiple dates, separate each date in a row (the dates will have the same context) ==> PROBLEM7\n",
    "    - remove the contexts taht contain the words in terms_discarding_the_date (meaning that those are probably other dates) ==> Build a classifier 3 classes after\n",
    "    - Return a list of tuples (context in lower + no stopwords + clean from punct , date)\n",
    "    - this context can be passed to Spacy avg vectorizer to get the avg Word embedding of the sentence\n",
    "    \"\"\"\n",
    "\n",
    "    l = [re.findall(\"\\d{1,2} [a-zéû]{3,9} \\d{4}\", STRING) for STRING in text]\n",
    "    indexes = [i for i in range(len(l)) if len(l[i])!=0]\n",
    "    ll = [(\" \".join(text[i-X+1 : i+X]), l[i]) for i in indexes]\n",
    "\n",
    "    lll = []\n",
    "    for i in range(len(ll)) :\n",
    "        if len([word for word in terms_discarding_the_date if word in ll[i][0]]) > 0 :\n",
    "            if len(ll[i][1]) == 1 :\n",
    "                date = ll[i][1][0]\n",
    "                context_date_removed = ll[i][0].replace(date, '')\n",
    "                lll.append((context_date_removed, date))\n",
    "    ind = np.random.choice(range(len(lll))) # je ne prends qu'un contexte aleatoirement pour commencer\n",
    "    return lll[ind][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autre dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for i in range(X_train.shape[0]) :\n",
    "    context_to_analyse = extract_X_sentences_around_all_dates_other_dates(X_train.text_processed.iloc[i])\n",
    "    \n",
    "    context_to_analyse_cleaned = \"\"\n",
    "    for element in context_to_analyse :\n",
    "        if not element.isalnum():\n",
    "            if element == \" \" :\n",
    "                context_to_analyse_cleaned += element \n",
    "        else :\n",
    "            context_to_analyse_cleaned += element \n",
    "    L.append(context_to_analyse_cleaned)\n",
    "    \n",
    "X_train[\"context_around_autre_date_random\"] = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import GradientBoostingRegressor\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom pathlib import Path\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_tr, X_te = train_test_split(X_train, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accident_context_avg_WV = np.array([\n",
    "    nlp(sen).vector for sen in X_tr.context_around_accident if sen is not None ])\n",
    "\n",
    "train_consolidation_context_avg_WV = np.array([\n",
    "    nlp(sen).vector for sen in X_tr.context_around_consolidation if sen is not None ])\n",
    "\n",
    "train_other_context_avg_WV = np.array([\n",
    "    nlp(sen).vector for sen in X_tr.context_around_autre_date_random if sen is not None ])\n",
    "\n",
    "data = np.concatenate((train_accident_context_avg_WV, train_consolidation_context_avg_WV, train_other_context_avg_WV))\n",
    "\n",
    "is_accident = [0 for _ in range(train_accident_context_avg_WV.shape[0])]\n",
    "is_condo = [1 for _ in range(train_consolidation_context_avg_WV.shape[0])]\n",
    "is_other = [2 for _ in range(train_other_context_avg_WV.shape[0])]\n",
    "\n",
    "target = np.concatenate((is_accident, is_condo, is_other))\n",
    "\n",
    "# # clf_accident = LogisticRegression().fit(data, target)\n",
    "clf = SVC(gamma='auto', probability=True).fit(data, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_not_none_accident = [ind for ind in X_te.index if X_te.context_around_accident.loc[ind] is not None]\n",
    "index_not_none_conso = [ind for ind in X_te.index if X_te.context_around_consolidation.loc[ind] is not None]\n",
    "\n",
    "\n",
    "y_true_accident = y_train.loc[index_not_none_accident].date_accident.values\n",
    "y_true_consolidation = y_train.loc[index_not_none_conso].date_consolidation.values\n",
    "\n",
    "y_pred_accident = {}\n",
    "y_pred_conso = {}\n",
    "\n",
    "for i in X_te.index :\n",
    "    \n",
    "    if (i in index_not_none_accident) & (i in index_not_none_conso) :\n",
    "        \n",
    "        sentences_to_test = extract_X_sentences_around_all_dates(X_te.text_processed.loc[i], [])\n",
    "    \n",
    "        probas_predictions = []\n",
    "        for tup in sentences_to_test :\n",
    "            processed_sentence = tup[0]\n",
    "            WV_processed_sentence = nlp(processed_sentence).vector\n",
    "            probas_predictions.append(clf.predict_proba([WV_processed_sentence]))\n",
    "        probas_predictions = np.squeeze(probas_predictions)\n",
    "\n",
    "        ind_accident = np.argmax(probas_predictions[:,0])\n",
    "        ind_conso = np.argmax(probas_predictions[:,1])    \n",
    "    \n",
    "        y_pred_accident[i] = letter_date_to_submission_date(sentences_to_test[ind_accident][1])\n",
    "        y_pred_conso[i] = letter_date_to_submission_date(sentences_to_test[ind_conso][1]) \n",
    "        \n",
    "    elif i in index_not_none_accident :\n",
    "        sentences_to_test = extract_X_sentences_around_all_dates(X_te.text_processed.loc[i], [])\n",
    "        probas_predictions = []\n",
    "        for tup in sentences_to_test :\n",
    "            processed_sentence = tup[0]\n",
    "            WV_processed_sentence = nlp(processed_sentence).vector\n",
    "            probas_predictions.append(clf.predict_proba([WV_processed_sentence]))\n",
    "        probas_predictions = np.squeeze(probas_predictions)\n",
    "\n",
    "        ind_accident = np.argmax(probas_predictions[:,0])    \n",
    "        y_pred_accident[i] = letter_date_to_submission_date(sentences_to_test[ind_accident][1])\n",
    "    \n",
    "    \n",
    "    elif i in index_not_none_conso :\n",
    "        sentences_to_test = extract_X_sentences_around_all_dates(X_te.text_processed.loc[i], [])\n",
    "        probas_predictions = []\n",
    "        for tup in sentences_to_test :\n",
    "            processed_sentence = tup[0]\n",
    "            WV_processed_sentence = nlp(processed_sentence).vector\n",
    "            probas_predictions.append(clf.predict_proba([WV_processed_sentence]))\n",
    "        probas_predictions = np.squeeze(probas_predictions)\n",
    "\n",
    "        ind_conso = np.argmax(probas_predictions[:,1])   \n",
    "        y_pred_conso[i] = letter_date_to_submission_date(sentences_to_test[ind_conso][1]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6190476190476191"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_accident == np.array(list(y_pred_accident.values()))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_consolidation == np.array(list(y_pred_conso.values()))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf idf approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accident_corpus = [sen for sen in X_tr.context_around_accident if sen is not None ]\n",
    "train_consolidation_corpus = [sen for sen in X_tr.context_around_consolidation if sen is not None ]\n",
    "train_other_context_corpus = [sen for sen in X_tr.context_around_autre_date_random if sen is not None]\n",
    "\n",
    "data = np.concatenate((train_accident_corpus, train_consolidation_corpus, train_other_context_corpus))\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_tfidf = vectorizer.fit_transform(data)\n",
    "\n",
    "is_accident = [0 for _ in range(len(train_accident_corpus))]\n",
    "is_condo = [1 for _ in range(len(train_consolidation_corpus))]\n",
    "is_other = [2 for _ in range(len(train_other_context_corpus))]\n",
    "\n",
    "target = np.concatenate((is_accident, is_condo, is_other))\n",
    "\n",
    "clf = SVC(gamma='auto', probability=True).fit(data_tfidf, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_not_none_accident = [ind for ind in X_te.index if X_te.context_around_accident.loc[ind] is not None]\n",
    "index_not_none_conso = [ind for ind in X_te.index if X_te.context_around_consolidation.loc[ind] is not None]\n",
    "\n",
    "\n",
    "y_true_accident = y_train.loc[index_not_none_accident].date_accident.values\n",
    "y_true_consolidation = y_train.loc[index_not_none_conso].date_consolidation.values\n",
    "\n",
    "y_pred_accident = {}\n",
    "y_pred_conso = {}\n",
    "\n",
    "for i in X_te.index :\n",
    "    \n",
    "    if (i in index_not_none_accident) & (i in index_not_none_conso) :\n",
    "        \n",
    "        sentences_to_test = extract_X_sentences_around_all_dates(X_te.text_processed.loc[i], [])\n",
    "    \n",
    "        probas_predictions = []\n",
    "        for tup in sentences_to_test :\n",
    "            processed_sentence = tup[0]\n",
    "            tfidf_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "            probas_predictions.append(clf.predict_proba(tfidf_processed_sentence))\n",
    "        probas_predictions = np.squeeze(probas_predictions)\n",
    "\n",
    "        ind_accident = np.argmax(probas_predictions[:,0])\n",
    "        ind_conso = np.argmax(probas_predictions[:,1])    \n",
    "    \n",
    "        y_pred_accident[i] = letter_date_to_submission_date(sentences_to_test[ind_accident][1])\n",
    "        y_pred_conso[i] = letter_date_to_submission_date(sentences_to_test[ind_conso][1]) \n",
    "        \n",
    "    elif i in index_not_none_accident :\n",
    "        sentences_to_test = extract_X_sentences_around_all_dates(X_te.text_processed.loc[i], [])\n",
    "        probas_predictions = []\n",
    "        for tup in sentences_to_test :\n",
    "            processed_sentence = tup[0]\n",
    "            tfidf_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "            probas_predictions.append(clf.predict_proba(tfidf_processed_sentence))\n",
    "        probas_predictions = np.squeeze(probas_predictions)\n",
    "\n",
    "        ind_accident = np.argmax(probas_predictions[:,0])    \n",
    "        y_pred_accident[i] = letter_date_to_submission_date(sentences_to_test[ind_accident][1])\n",
    "    \n",
    "    \n",
    "    elif i in index_not_none_conso :\n",
    "        sentences_to_test = extract_X_sentences_around_all_dates(X_te.text_processed.loc[i], [])\n",
    "        probas_predictions = []\n",
    "        for tup in sentences_to_test :\n",
    "            processed_sentence = tup[0]\n",
    "            tfidf_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "            probas_predictions.append(clf.predict_proba(tfidf_processed_sentence))\n",
    "        probas_predictions = np.squeeze(probas_predictions)\n",
    "\n",
    "        ind_conso = np.argmax(probas_predictions[:,1])   \n",
    "        y_pred_conso[i] = letter_date_to_submission_date(sentences_to_test[ind_conso][1]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_accident == np.array(list(y_pred_accident.values()))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_consolidation == np.array(list(y_pred_conso.values()))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

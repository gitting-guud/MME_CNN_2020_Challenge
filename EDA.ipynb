{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from preprocessing import *\n",
    "from models_date import *\n",
    "from models_sex_prediction import *\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data ...\n",
      "Reading training data : Done\n",
      "Reading testing data ...\n",
      "Reading testing data : Done\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test = read_train_test(sentence_per_row_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "X_train[\"text_processed\"] = X_train.text.apply(process_text, args=(False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agen_100515.txt</td>\n",
       "      <td>[Le : 12/11/2019, , , Cour d’appel d’Agen , , ...</td>\n",
       "      <td>[: 12/11/2019, , , cour appel agen, , chambre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agen_1100752.txt</td>\n",
       "      <td>[Le : 12/11/2019, , , Cour d’appel d’Agen , , ...</td>\n",
       "      <td>[: 12/11/2019, , , cour appel agen, , chambre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Agen_1613.txt</td>\n",
       "      <td>[Le : 12/11/2019, , , Cour d’appel d’Agen , , ...</td>\n",
       "      <td>[: 12/11/2019, , , cour appel agen, , audience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agen_2118.txt</td>\n",
       "      <td>[Le : 12/11/2019, , , Cour d’appel d’Agen , , ...</td>\n",
       "      <td>[: 12/11/2019, , , cour appel agen, , audience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agen_21229.txt</td>\n",
       "      <td>[Le : 12/11/2019, , , Cour d’appel d’Agen , , ...</td>\n",
       "      <td>[: 12/11/2019, , , cour appel agen, , audience...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename                                               text  \\\n",
       "ID                                                                        \n",
       "0    Agen_100515.txt  [Le : 12/11/2019, , , Cour d’appel d’Agen , , ...   \n",
       "1   Agen_1100752.txt  [Le : 12/11/2019, , , Cour d’appel d’Agen , , ...   \n",
       "2      Agen_1613.txt  [Le : 12/11/2019, , , Cour d’appel d’Agen , , ...   \n",
       "3      Agen_2118.txt  [Le : 12/11/2019, , , Cour d’appel d’Agen , , ...   \n",
       "4     Agen_21229.txt  [Le : 12/11/2019, , , Cour d’appel d’Agen , , ...   \n",
       "\n",
       "                                       text_processed  \n",
       "ID                                                     \n",
       "0   [: 12/11/2019, , , cour appel agen, , chambre ...  \n",
       "1   [: 12/11/2019, , , cour appel agen, , chambre ...  \n",
       "2   [: 12/11/2019, , , cour appel agen, , audience...  \n",
       "3   [: 12/11/2019, , , cour appel agen, , audience...  \n",
       "4   [: 12/11/2019, , , cour appel agen, , audience...  "
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction du contexte autour de la date (utilisant la target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [
     28,
     91,
     95
    ]
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# import igraph\n",
    "# import copy\n",
    "\n",
    "# def terms_to_graph(terms, window_size):\n",
    "#     '''This function returns a directed, weighted igraph from lists of list of terms (the tokens from the pre-processed text)\n",
    "#     e.g., ['quick','brown','fox']\n",
    "#     Edges are weighted based on term co-occurence within a sliding window of fixed size 'w'\n",
    "#     '''\n",
    "    \n",
    "#     from_to = {}\n",
    "\n",
    "#     w = min(window_size, len(terms))\n",
    "#     # create initial complete graph (first w terms)\n",
    "#     terms_temp = terms[0:w]\n",
    "#     indexes = list(itertools.combinations(range(w), r=2))\n",
    "\n",
    "#     new_edges = []\n",
    "\n",
    "#     for my_tuple in indexes:\n",
    "#         new_edges.append(tuple([terms_temp[i] for i in my_tuple]))\n",
    "#     for new_edge in new_edges:\n",
    "#         if new_edge in from_to:\n",
    "#             from_to[new_edge] += 1\n",
    "#         else:\n",
    "#             from_to[new_edge] = 1\n",
    "\n",
    "#     # then iterate over the remaining terms\n",
    "#     for i in range(w, len(terms)):\n",
    "#         # term to consider\n",
    "#         considered_term = terms[i]\n",
    "#         # all terms within sliding window\n",
    "#         terms_temp = terms[(i - w + 1):(i + 1)]\n",
    "\n",
    "#         # edges to try\n",
    "#         candidate_edges = []\n",
    "#         for p in range(w - 1):\n",
    "#             candidate_edges.append((terms_temp[p], considered_term))\n",
    "\n",
    "#         for try_edge in candidate_edges:\n",
    "\n",
    "#             # if not self-edge\n",
    "#             if try_edge[1] != try_edge[0]:\n",
    "\n",
    "#                 # if edge has already been seen, update its weight\n",
    "#                 if try_edge in from_to:\n",
    "#                     from_to[try_edge] += 1\n",
    "\n",
    "#                 # if edge has never been seen, create it and assign it a unit weight\n",
    "#                 else:\n",
    "#                     from_to[try_edge] = 1\n",
    "\n",
    "#     # create empty graph\n",
    "#     g = igraph.Graph(directed=True)\n",
    "\n",
    "#     # add vertices\n",
    "#     g.add_vertices(sorted(set(terms)))\n",
    "\n",
    "#     # add edges, direction is preserved since the graph is directed\n",
    "#     g.add_edges(list(from_to.keys()))\n",
    "\n",
    "#     # set edge and vertice weights\n",
    "#     g.es['weight'] = list(from_to.values()) # based on co-occurence within sliding window\n",
    "#     g.vs['weight'] = g.strength(weights=list(from_to.values())) # weighted degree\n",
    "\n",
    "#     return (g)\n",
    "\n",
    "# def core_dec(g,weighted):\n",
    "#     '''(un)weighted k-core decomposition'''\n",
    "#     # work on clone of g to preserve g \n",
    "#     gg = copy.deepcopy(g)\n",
    "#     if not weighted:\n",
    "#         gg.vs['weight'] = gg.strength() # overwrite the 'weight' vertex attribute with the unweighted degrees\n",
    "#     # initialize dictionary that will contain the core numbers\n",
    "#     cores_g = dict(zip(gg.vs['name'],[0]*len(gg.vs)))\n",
    "    \n",
    "#     while len(gg.vs) > 0:\n",
    "#         # find index of lowest degree vertex\n",
    "#         min_degree = min(gg.vs['weight'])\n",
    "#         index_top = gg.vs['weight'].index(min_degree)\n",
    "#         name_top = gg.vs[index_top]['name']\n",
    "#         # get names of its neighbors\n",
    "#         neighbors = gg.vs[gg.neighbors(index_top)]['name']\n",
    "#         # exclude self-edges\n",
    "#         neighbors = [elt for elt in neighbors if elt!=name_top]\n",
    "#         # set core number of lowest degree vertex as its degree\n",
    "#         cores_g[name_top] = min_degree\n",
    "#         # delete top vertex and its incident edges\n",
    "#         gg.delete_vertices(index_top)\n",
    "        \n",
    "#         if neighbors:\n",
    "#             if weighted: \n",
    "#                 new_degrees = gg.strength(weights=gg.es['weight'])\n",
    "#             else:\n",
    "#                 new_degrees = gg.strength()\n",
    "#             # iterate over neighbors of top element\n",
    "#             for neigh in neighbors:\n",
    "#                 index_n = gg.vs['name'].index(neigh)\n",
    "#                 gg.vs[index_n]['weight'] = max(min_degree,new_degrees[index_n])  \n",
    "        \n",
    "#     return(cores_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_to_analyse = extract_X_sentences_before_after(X_train.text_processed[20], y_train.date_accident[20], X=1)\n",
    "\n",
    "\n",
    "# context_to_analyse = extract_X_sentences_around_all_dates(X_train.text_processed[20], \n",
    "#                                                           terms_discarding_the_date, \n",
    "#                                                           use_date_forcing=True,\n",
    "#                                                           date_to_look_for=y_train.date_accident[20])\n",
    "# context_to_analyse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Contexte autour de la date d':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Renvoie `None`  si : \n",
    "    - La date dans y n'a pas été trouvée sous le format jour (nombre ou \"1er\") mois (en letter) année : 11 decembre 2007 | 1er avril 2006\n",
    "    - La date est n.c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for i in range(X_train.shape[0]) :\n",
    "    context_to_analyse = extract_X_sentences_around_all_dates(X_train.text_processed.iloc[i], \n",
    "                                                              terms_discarding_the_date, \n",
    "                                                              use_date_forcing=True,\n",
    "                                                              date_to_look_for=y_train.date_accident.iloc[i])\n",
    "    if (context_to_analyse == []) or context_to_analyse == \"Date NC\" :\n",
    "        L.append(None)\n",
    "\n",
    "    else :\n",
    "        L.append(context_to_analyse[0][0]) # je ne prends que le premier contexte pour commencer\n",
    "#         L.append(\" \".join(np.array(context_to_analyse)[:,0]))\n",
    "X_train[\"context_around_accident\"] = L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for i in range(X_train.shape[0]) :\n",
    "    context_to_analyse = extract_X_sentences_around_all_dates(X_train.text_processed.iloc[i], \n",
    "                                                              terms_discarding_the_date, \n",
    "                                                              use_date_forcing=True,\n",
    "                                                              date_to_look_for=y_train.date_consolidation.iloc[i])\n",
    "    if (context_to_analyse == []) or context_to_analyse == \"Date NC\" :\n",
    "        L.append(None)\n",
    "    else :\n",
    "        L.append(context_to_analyse[0][0]) # je ne prends que le premier contexte pour commencer\n",
    "#         L.append(\" \".join(np.array(context_to_analyse)[:,0])) # concatener tout les contextes\n",
    "X_train[\"context_around_consolidation\"] = L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from pathlib import Path\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_tr, X_te = train_test_split(X_train, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To asses the performance of our model that predicts if the context around a date is talking about `date d'accident` or `date de consolidation` :\n",
    "\n",
    "- Take split X_train using train_test_split into X_tr and X_te\n",
    "- Prepare data for model fitting :\n",
    "    - For date d'accident : \n",
    "        - remove the dates from the accident contexts of X_tr\n",
    "        - remove the punctuation from the accident contexts of X_tr\n",
    "        - remove the stopwords from the accident contexts of X_tr  \n",
    "        - use Spacy avg vectorizer or tfidf\n",
    "    - Same thing for date de conso\n",
    "    - concatenate the two \n",
    "    - fit two models : one to predict accident contexts and the other the consolidation contexts\n",
    "- On X_te :\n",
    "    - On each text apply the function extract `X_sentences_around_all_dates`\n",
    "    - for each sentence apply the Spacy vectorizer and predict using the models\n",
    "    - predict the date of which the context gave the higher probability score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results : \n",
    "- accident : 35% accuracy with tf idf approach on context extracted with X=1 and SVM classifier\n",
    "- conso : 24% accuracy with spacy word vectors approach on context extracted with X=1 and SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_accident_context_avg_WV = np.array([\n",
    "#     nlp(sen).vector for sen in X_tr.context_around_accident_processed if sen is not None ])\n",
    "\n",
    "# train_consolidation_context_avg_WV = np.array([\n",
    "#     nlp(sen).vector for sen in X_tr.context_around_consolidation_processed if sen is not None ])\n",
    "\n",
    "train_accident_context_avg_WV = np.array([\n",
    "    nlp(sen).vector for sen in X_tr.context_around_accident if sen is not None ])\n",
    "\n",
    "train_consolidation_context_avg_WV = np.array([\n",
    "    nlp(sen).vector for sen in X_tr.context_around_consolidation if sen is not None ])\n",
    "\n",
    "\n",
    " \n",
    "is_accident = [1 for _ in range(train_accident_context_avg_WV.shape[0])]\n",
    "is_not_accident = [0 for _ in range(train_consolidation_context_avg_WV.shape[0])]\n",
    "data = np.concatenate((train_accident_context_avg_WV, train_consolidation_context_avg_WV))\n",
    "target = np.concatenate((is_accident, is_not_accident))\n",
    "\n",
    "# clf_accident = LogisticRegression().fit(data, target)\n",
    "clf_accident = SVC(gamma='auto', probability=True).fit(data, target)\n",
    "\n",
    "###############################################\n",
    "\n",
    "is_conso = [1 for _ in range(train_consolidation_context_avg_WV.shape[0])]\n",
    "is_not_conso = [0 for _ in range(train_accident_context_avg_WV.shape[0])]\n",
    "data = np.concatenate((train_consolidation_context_avg_WV, train_accident_context_avg_WV))\n",
    "target = np.concatenate((is_conso, is_not_conso))\n",
    "\n",
    "# clf_conso = LogisticRegression().fit(data, target)\n",
    "clf_conso = SVC(gamma='auto', probability=True).fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word_vector_SPACY(text) :\n",
    "    \"\"\"For a given context : gives the average spacy word vectors of the context\n",
    "    used in df.apply() of a 'context_around_...' column of a dataframe\n",
    "    \"\"\"\n",
    "    if text is None :\n",
    "        return None\n",
    "    else :\n",
    "        return nlp(text).vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accident_context_avg_WV = X_tr.context_around_accident.apply(Word_vector_SPACY)\n",
    "train_consolidation_context_avg_WV = X_tr.context_around_consolidation.apply(Word_vector_SPACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_is_accident_context = np.matrix(train_accident_context_avg_WV[train_accident_context_avg_WV.isnull() == False].to_list())\n",
    "data_is_consolidation_context = np.matrix(train_consolidation_context_avg_WV[train_consolidation_context_avg_WV.isnull() == False].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((data_is_accident_context, data_is_consolidation_context))\n",
    "target_accident = np.concatenate(([1 for _ in range(data_is_accident_context.shape[0])],\n",
    "                                 [0 for _ in range(data_is_consolidation_context.shape[0])]))\n",
    "\n",
    "target_consolidation = np.concatenate(([0 for _ in range(data_is_accident_context.shape[0])],\n",
    "                                        [1 for _ in range(data_is_consolidation_context.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_accident_context_avg_WV[train_accident_context_avg_WV.isnull() == False].index\n",
    "# train_consolidation_context_avg_WV[train_consolidation_context_avg_WV.isnull() == False].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_accident = SVC(gamma='auto', probability=True).fit(data, target_accident)\n",
    "clf_consolidation = SVC(gamma='auto', probability=True).fit(data, target_consolidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def date_prediction_classifier_Word_vectors(text, clf, threshold=0.7):\n",
    "    \n",
    "    probas_ = []\n",
    "    sentences_to_test = extract_X_sentences_around_all_dates(text, terms_discarding_the_date)\n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        WV_processed_sentence = nlp(processed_sentence).vector\n",
    "        probas_.append(clf.predict_proba([WV_processed_sentence]))\n",
    "\n",
    "    probas_ = np.array(probas_)\n",
    "    if probas_.shape[0] == 1 :\n",
    "        probas_ = probas_[0]\n",
    "    else :\n",
    "        probas_ = np.squeeze(probas_)[:,1]\n",
    "    \n",
    "    if probas_.max() >= threshold :\n",
    "        return letter_date_to_submission_date(sentences_to_test[np.argmax(probas_)][1])\n",
    "    else :\n",
    "        return \"n.c.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_accident = X_te.text_processed.apply(date_prediction_classifier_Word_vectors, args=(clf_accident,))\n",
    "y_pred_consolidation = X_te.text_processed.apply(date_prediction_classifier_Word_vectors, args=(clf_consolidation,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_accident = y_train.date_accident.loc[X_te.index]\n",
    "y_true_consolidation = y_train.date_consolidation.loc[X_te.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49740932642487046, 0.41968911917098445)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_accident == y_pred_accident).mean(), (y_pred_consolidation == y_true_consolidation).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf idf approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_is_accident_corpus = X_tr.context_around_accident[X_tr.context_around_accident.isnull() == False].to_list()\n",
    "data_is_consolidation_corpus = X_tr.context_around_consolidation[X_tr.context_around_consolidation.isnull() == False].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.concatenate((data_is_accident_corpus, data_is_consolidation_corpus))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_tfidf = vectorizer.fit_transform(data)\n",
    "\n",
    "target_accident = np.concatenate(([1 for _ in range(len(data_is_accident_corpus))],\n",
    "                                 [0 for _ in range(len(data_is_consolidation_corpus))]))\n",
    "\n",
    "target_consolidation = np.concatenate(([0 for _ in range(len(data_is_accident_corpus))],\n",
    "                                        [1 for _ in range(len(data_is_consolidation_corpus))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_accident = SVC(gamma='auto', probability=True).fit(data_tfidf, target_accident)\n",
    "clf_consolidation = SVC(gamma='auto', probability=True).fit(data_tfidf, target_consolidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def date_prediction_classifier_tfidf(text, clf, vectorizer, threshold=0.7):\n",
    "    \n",
    "    probas_ = []\n",
    "    sentences_to_test = extract_X_sentences_around_all_dates(text, terms_discarding_the_date)\n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        tfidf_encoded_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "        probas_.append(clf.predict_proba(tfidf_encoded_processed_sentence))\n",
    "\n",
    "    probas_ = np.array(probas_)\n",
    "    if probas_.shape[0] == 1 :\n",
    "        probas_ = probas_[0][0]\n",
    "        if probas_.max() >= threshold :\n",
    "            return letter_date_to_submission_date(sentences_to_test[0][1])\n",
    "        else :\n",
    "            return \"n.c.\"\n",
    "        \n",
    "    else :\n",
    "        probas_ = np.squeeze(probas_)[:,1]\n",
    "\n",
    "        if probas_.max() >= threshold :\n",
    "            return letter_date_to_submission_date(sentences_to_test[np.argmax(probas_)][1])\n",
    "        else :\n",
    "            return \"n.c.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_accident = X_te.text_processed.apply(date_prediction_classifier_tfidf, args=(clf_accident, vectorizer,))\n",
    "y_pred_consolidation = X_te.text_processed.apply(date_prediction_classifier_tfidf, args=(clf_consolidation, vectorizer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_accident = y_train.date_accident.loc[X_te.index]\n",
    "y_true_consolidation = y_train.date_consolidation.loc[X_te.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5906735751295337, 0.5025906735751295)"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_accident == y_pred_accident).mean(), (y_pred_consolidation == y_true_consolidation).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three classes approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_discarding_the_date = [\"loi\",\n",
    "                             \"jugement\",\n",
    "                             \"audience\",\n",
    "                             \"publique\",\n",
    "#                              \"juge\",\n",
    "                             \"tribubal\",\n",
    "                             \"decision\",\n",
    "                             \"greffe\",\n",
    "                             \"conclusion\",\n",
    "                             \"ordonnance\",\n",
    "                             ]\n",
    "def extract_X_sentences_around_all_dates_other_dates(text, terms_discarding_the_date= terms_discarding_the_date, \n",
    "                                                     X=1):\n",
    "    \"\"\"\n",
    "    The text in structured format ( like in the column text of X_train)\n",
    "    - Looks for all the dates in the text\n",
    "    - for each date, extract X sentences  before and after this date (sentence meaning rows of the original doc)\n",
    "    - if a row has multiple dates, separate each date in a row (the dates will have the same context) ==> PROBLEM7\n",
    "    - remove the contexts taht contain the words in terms_discarding_the_date (meaning that those are probably other dates) ==> Build a classifier 3 classes after\n",
    "    - Return a list of tuples (context in lower + no stopwords + clean from punct , date)\n",
    "    - this context can be passed to Spacy avg vectorizer to get the avg Word embedding of the sentence\n",
    "    \"\"\"\n",
    "\n",
    "    l = [re.findall(\"\\d{1,2} [a-zéû]{3,9} \\d{4}\", STRING) for STRING in text]\n",
    "    indexes = [i for i in range(len(l)) if len(l[i])!=0]\n",
    "    ll = [(\" \".join(text[i-X+1 : i+X]), l[i]) for i in indexes]\n",
    "\n",
    "    lll = []\n",
    "    for i in range(len(ll)) :\n",
    "        if len([word for word in terms_discarding_the_date if word in ll[i][0]]) > 0 :\n",
    "            if len(ll[i][1]) == 1 :\n",
    "                date = ll[i][1][0]\n",
    "                context_date_removed = ll[i][0].replace(date, '')\n",
    "                lll.append((context_date_removed, date))\n",
    "                \n",
    "#     return \" \".join([lll[ind][0] for ind in range(len(lll))])\n",
    "\n",
    "    ind = np.random.choice(range(len(lll))) # je ne prends qu'un contexte aleatoirement pour commencer\n",
    "    return lll[ind][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autre dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "for i in range(X_train.shape[0]) :\n",
    "    context_to_analyse = extract_X_sentences_around_all_dates_other_dates(X_train.text_processed.iloc[i])\n",
    "    \n",
    "    context_to_analyse_cleaned = \"\"\n",
    "    for element in context_to_analyse :\n",
    "        if not element.isalnum():\n",
    "            if element == \" \" :\n",
    "                context_to_analyse_cleaned += element \n",
    "        else :\n",
    "            context_to_analyse_cleaned += element \n",
    "    L.append(context_to_analyse_cleaned)\n",
    "    \n",
    "X_train[\"context_around_autre_date_random\"] = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from pathlib import Path\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import train_test_split'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_tr, X_te = train_test_split(X_train, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Word vectors approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_accident_context_avg_WV = X_tr.context_around_accident.apply(Word_vector_SPACY)\n",
    "train_consolidation_context_avg_WV = X_tr.context_around_consolidation.apply(Word_vector_SPACY)\n",
    "train_other_context_avg_WV = X_tr.context_around_autre_date_random.apply(Word_vector_SPACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_accident_context = np.matrix(train_accident_context_avg_WV[train_accident_context_avg_WV.isnull() == False].to_list())\n",
    "data_consolidation_context = np.matrix(train_consolidation_context_avg_WV[train_consolidation_context_avg_WV.isnull() == False].to_list())\n",
    "data_other_context = np.matrix(train_other_context_avg_WV[train_other_context_avg_WV.isnull() == False].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data = np.concatenate((data_accident_context, data_consolidation_context, data_other_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "is_accident = [0 for _ in range(data_accident_context.shape[0])]\n",
    "is_conso = [1 for _ in range(data_consolidation_context.shape[0])]\n",
    "is_other = [2 for _ in range(data_other_context.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target = np.concatenate((is_accident, is_conso, is_other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto', probability=True).fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def date_prediction_classifier_Word_vectors_multiclass(text, clf, threshold=0.35):\n",
    "    \n",
    "    probas_ = []\n",
    "    sentences_to_test = extract_X_sentences_around_all_dates(text, terms_discarding_the_date)\n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        WV_processed_sentence = nlp(processed_sentence).vector\n",
    "        probas_.append(clf.predict_proba([WV_processed_sentence]))\n",
    "\n",
    "    probas_ = np.array(probas_)\n",
    "    if probas_.shape[0] == 1 :\n",
    "        probas_ = probas_[0][0]\n",
    "        if np.argmax(probas_) == 0 :\n",
    "            return [letter_date_to_submission_date(sentences_to_test[0][1]), \"n.c.\"]\n",
    "        elif np.argmax(probas_) == 1 :\n",
    "            return [\"n.c.\", letter_date_to_submission_date(sentences_to_test[0][1])] \n",
    "        else :\n",
    "            return [\"n.c.\", \"n.c.\"]\n",
    "    else :\n",
    "        probas_ = np.squeeze(probas_)\n",
    "        \n",
    "        event = [\"accident\", \"conso\"]\n",
    "        decisions = []\n",
    "        for row in probas_ :\n",
    "            if np.argmax(row) == 2 :\n",
    "                decisions.append(\"other\")\n",
    "            elif max(row[:2])<= threshold :\n",
    "                decisions.append(\"other\")\n",
    "            else :\n",
    "                decisions.append(event[np.argmax(row[:2])])\n",
    "        \n",
    "        ind_accident = None\n",
    "        ind_conso = None\n",
    "        prev_prob_acc = 0\n",
    "        prev_prob_conso = 0\n",
    "        \n",
    "        for i in range(probas_.shape[0]):\n",
    "            prob_acc = probas_[i,0]\n",
    "            if (decisions[i] == \"accident\") & (prob_acc >= prev_prob_acc) :\n",
    "                ind_accident = i\n",
    "                prev_prob_acc = prob_acc\n",
    "                \n",
    "            prob_conso = probas_[i,1]\n",
    "            if (decisions[i] == \"conso\") & (prob_conso >= prev_prob_conso):\n",
    "                ind_conso = i\n",
    "                prev_prob_conso = prob_conso\n",
    "                \n",
    "        if ind_accident is None :\n",
    "            date_accident = \"n.c.\"\n",
    "        else :\n",
    "            date_accident = letter_date_to_submission_date(sentences_to_test[ind_accident][1])\n",
    "            \n",
    "        if ind_conso is None :\n",
    "            date_conso = \"n.c.\"\n",
    "        else :\n",
    "            date_conso = letter_date_to_submission_date(sentences_to_test[ind_conso][1])\n",
    "        \n",
    "        return [date_accident, date_conso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = X_te.text_processed.apply(date_prediction_classifier_Word_vectors_multiclass, args=(clf,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred_accident = pd.DataFrame(y_pred.values.tolist(), index= y_pred.index)[0]\n",
    "y_pred_consolidation = pd.DataFrame(y_pred.values.tolist(), index= y_pred.index)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_true_accident = y_train.date_accident.loc[X_te.index]\n",
    "y_true_consolidation = y_train.date_consolidation.loc[X_te.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5492227979274611, 0.38341968911917096)"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_accident == y_pred_accident).mean(), (y_pred_consolidation == y_true_consolidation).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf idf approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def date_prediction_classifier_tfidf_multiclass(text, clf, vectorizer, threshold=0.35):\n",
    "    \n",
    "    probas_ = []\n",
    "    sentences_to_test = extract_X_sentences_around_all_dates(text, terms_discarding_the_date=[])\n",
    "    for tup in sentences_to_test :\n",
    "        processed_sentence = tup[0]\n",
    "        tfidf_encoded_processed_sentence = vectorizer.transform([processed_sentence])\n",
    "        probas_.append(clf.predict_proba(tfidf_encoded_processed_sentence))\n",
    "\n",
    "    probas_ = np.array(probas_)\n",
    "    if probas_.shape[0] == 1 :\n",
    "        probas_ = probas_[0][0]\n",
    "        if np.argmax(probas_) == 0 :\n",
    "            return [letter_date_to_submission_date(sentences_to_test[0][1]), \"n.c.\"]\n",
    "        elif np.argmax(probas_) == 1 :\n",
    "            return [\"n.c.\", letter_date_to_submission_date(sentences_to_test[0][1])] \n",
    "        else :\n",
    "            return [\"n.c.\", \"n.c.\"]\n",
    "    else :\n",
    "        probas_ = np.squeeze(probas_)\n",
    "        \n",
    "        event = [\"accident\", \"conso\"]\n",
    "        decisions = []\n",
    "        for row in probas_ :\n",
    "            if np.argmax(row) == 2 :\n",
    "                decisions.append(\"other\")\n",
    "            elif max(row[:2])<= threshold :\n",
    "                decisions.append(\"other\")\n",
    "            else :\n",
    "                decisions.append(event[np.argmax(row[:2])])\n",
    "        \n",
    "        ind_accident = None\n",
    "        ind_conso = None\n",
    "        prev_prob_acc = 0\n",
    "        prev_prob_conso = 0\n",
    "        \n",
    "        for i in range(probas_.shape[0]):\n",
    "            prob_acc = probas_[i,0]\n",
    "            if (decisions[i] == \"accident\") & (prob_acc >= prev_prob_acc) :\n",
    "                ind_accident = i\n",
    "                prev_prob_acc = prob_acc\n",
    "                \n",
    "            prob_conso = probas_[i,1]\n",
    "            if (decisions[i] == \"conso\") & (prob_conso >= prev_prob_conso):\n",
    "                ind_conso = i\n",
    "                prev_prob_conso = prob_conso\n",
    "                \n",
    "        if ind_accident is None :\n",
    "            date_accident = \"n.c.\"\n",
    "        else :\n",
    "            date_accident = letter_date_to_submission_date(sentences_to_test[ind_accident][1])\n",
    "            \n",
    "        if ind_conso is None :\n",
    "            date_conso = \"n.c.\"\n",
    "        else :\n",
    "            date_conso = letter_date_to_submission_date(sentences_to_test[ind_conso][1])\n",
    "        \n",
    "        return [date_accident, date_conso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_is_accident_corpus = X_tr.context_around_accident[X_tr.context_around_accident.isnull() == False].to_list()\n",
    "data_is_consolidation_corpus = X_tr.context_around_consolidation[X_tr.context_around_consolidation.isnull() == False].to_list()\n",
    "data_other_context = X_tr.context_around_autre_date_random[X_tr.context_around_autre_date_random.isnull() == False].to_list()\n",
    "data = np.concatenate((data_is_accident_corpus, data_is_consolidation_corpus, data_other_context))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_tfidf = vectorizer.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_accident = [0 for _ in range(len(data_is_accident_corpus))]\n",
    "is_conso = [1 for _ in range(len(data_is_consolidation_corpus))]\n",
    "is_other = [2 for _ in range(len(data_other_context))]\n",
    "target = np.concatenate((is_accident, is_conso, is_other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto', probability=True).fit(data_tfidf, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_te.text_processed.apply(date_prediction_classifier_tfidf_multiclass, args=(clf, vectorizer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_accident = pd.DataFrame(y_pred.values.tolist(), index= y_pred.index)[0]\n",
    "y_pred_consolidation = pd.DataFrame(y_pred.values.tolist(), index= y_pred.index)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_accident = y_train.date_accident.loc[X_te.index]\n",
    "y_true_consolidation = y_train.date_consolidation.loc[X_te.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6528497409326425, 0.44041450777202074)"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true_accident == y_pred_accident).mean(), (y_pred_consolidation == y_true_consolidation).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive sex predictor version 1 \n",
    "- accident : tfidf 3 classes\n",
    "- consolidation : tf idf 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_prediction = X_test.text.apply(naive_sex_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"text_processed\"] = X_test.text.apply(process_text, args=(False,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_is_accident_corpus = X_train.context_around_accident[X_train.context_around_accident.isnull() == False].to_list()\n",
    "data_is_consolidation_corpus = X_train.context_around_consolidation[X_train.context_around_consolidation.isnull() == False].to_list()\n",
    "data = np.concatenate((data_is_accident_corpus, data_is_consolidation_corpus))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_tfidf = vectorizer.fit_transform(data)\n",
    "\n",
    "\n",
    "target_consolidation = np.concatenate(([0 for _ in range(len(data_is_accident_corpus))],\n",
    "                                        [1 for _ in range(len(data_is_consolidation_corpus))]))\n",
    "clf_consolidation = SVC(gamma='auto', probability=True).fit(data_tfidf, target_consolidation)\n",
    "y_pred_consolidation = X_test.text_processed.apply(date_prediction_classifier_tfidf, args=(clf_consolidation, vectorizer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_is_accident_corpus = X_train.context_around_accident[X_train.context_around_accident.isnull() == False].to_list()\n",
    "data_is_consolidation_corpus = X_train.context_around_consolidation[X_train.context_around_consolidation.isnull() == False].to_list()\n",
    "data_other_context = X_train.context_around_autre_date_random[X_train.context_around_autre_date_random.isnull() == False].to_list()\n",
    "data = np.concatenate((data_is_accident_corpus, data_is_consolidation_corpus, data_other_context))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_tfidf = vectorizer.fit_transform(data)\n",
    "\n",
    "is_accident = [0 for _ in range(len(data_is_accident_corpus))]\n",
    "is_conso = [1 for _ in range(len(data_is_consolidation_corpus))]\n",
    "is_other = [2 for _ in range(len(data_other_context))]\n",
    "target = np.concatenate((is_accident, is_conso, is_other))\n",
    "clf = SVC(gamma='auto', probability=True).fit(data_tfidf, target)\n",
    "y_pred = X_test.text_processed.apply(date_prediction_classifier_tfidf_multiclass, args=(clf, vectorizer,))\n",
    "y_pred_accident = pd.DataFrame(y_pred.values.tolist(), index= y_pred.index)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sexe</th>\n",
       "      <th>date_accident</th>\n",
       "      <th>date_consolidation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>homme</td>\n",
       "      <td>2014-10-07</td>\n",
       "      <td>n.c.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>homme</td>\n",
       "      <td>1996-07-05</td>\n",
       "      <td>n.c.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>femme</td>\n",
       "      <td>1989-05-11</td>\n",
       "      <td>n.c.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>homme</td>\n",
       "      <td>2001-01-25</td>\n",
       "      <td>n.c.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>homme</td>\n",
       "      <td>1999-05-28</td>\n",
       "      <td>2001-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>homme</td>\n",
       "      <td>2011-05-23</td>\n",
       "      <td>2005-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>femme</td>\n",
       "      <td>1968-05-20</td>\n",
       "      <td>1968-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>femme</td>\n",
       "      <td>1948-05-13</td>\n",
       "      <td>2012-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>homme</td>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>2002-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>homme</td>\n",
       "      <td>1987-04-29</td>\n",
       "      <td>n.c.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sexe date_accident date_consolidation\n",
       "filename                                        \n",
       "770       homme    2014-10-07               n.c.\n",
       "771       homme    1996-07-05               n.c.\n",
       "772       femme    1989-05-11               n.c.\n",
       "773       homme    2001-01-25               n.c.\n",
       "774       homme    1999-05-28         2001-05-28\n",
       "...         ...           ...                ...\n",
       "1022      homme    2011-05-23         2005-04-07\n",
       "1023      femme    1968-05-20         1968-05-20\n",
       "1024      femme    1948-05-13         2012-01-26\n",
       "1025      homme    2000-10-09         2002-09-30\n",
       "1026      homme    1987-04-29               n.c.\n",
       "\n",
       "[257 rows x 3 columns]"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.DataFrame(index = y_pred_accident.index)\n",
    "submit[\"sexe\"] = sex_prediction\n",
    "submit[\"date_accident\"] = y_pred_accident\n",
    "submit[\"date_consolidation\"] = y_pred_consolidation\n",
    "submit.index = submit.index.rename(\"filename\")\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(\"Sub1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
